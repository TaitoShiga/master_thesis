% ==============================
% 序章の参考文献
% ==============================

% Atariゲームでのモデルベース強化学習の成功例
@article{hafner2020mastering,
  title   = {Mastering atari with discrete world models},
  author  = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  journal = {arXiv preprint arXiv:2010.02193},
  year    = {2020}
}

% Control系の複雑タスクの成功例
@article{hafner2019dream,
  title   = {Dream to control: Learning behaviors by latent imagination},
  author  = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal = {arXiv preprint arXiv:1912.01603},
  year    = {2019}
}

@article{hafner2025mastering,
  title={Mastering diverse control tasks through world models},
  author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  journal={Nature},
  pages={1--7},
  year={2025},
  publisher={Nature Publishing Group UK London}
}

@article{lee2020stochastic,
  title   = {Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model},
  author  = {Lee, Alex X and Nagabandi, Anusha and Abbeel, Pieter and Levine, Sergey},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {741--752},
  year    = {2020}
}

% モデルフリーよりもサンプル効率がいいことの根拠

@inproceedings{NIPS2014_c7c9344b,
  author    = {Levine, Sergey and Abbeel, Pieter},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c7c9344b5a3c0533e29fa69ce807cf08-Paper.pdf},
  volume    = {27},
  year      = {2014}
}

% TDMPC2 のちの前提知識の章でも参照
@article{hansen2023td,
  title   = {Td-mpc2: Scalable, robust world models for continuous control},
  author  = {Hansen, Nicklas and Su, Hao and Wang, Xiaolong},
  journal = {arXiv preprint arXiv:2310.16828},
  year    = {2023}
}

% Domain Randomization の説明
@inproceedings{tobin2017domain,
  title        = {Domain randomization for transferring deep neural networks from simulation to the real world},
  author       = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle    = {2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)},
  pages        = {23--30},
  year         = {2017},
  organization = {IEEE}
}

% 強化学習の説明用
@book{sutton2018reinforcement,
  title     = {Reinforcement Learning, second edition: An Introduction},
  author    = {Sutton, R.S. and Barto, A.G.},
  isbn      = {9780262352703},
  series    = {Adaptive Computation and Machine Learning series},
  url       = {https://books.google.co.jp/books?id=uWV0DwAAQBAJ},
  year      = {2018},
  publisher = {MIT Press}
}

@article{markov,
  issn      = {00959057, 19435274},
  url       = {http://www.jstor.org/stable/24900506},
  author    = {R.Bellman},
  journal   = {Journal of Mathematics and Mechanics},
  number    = {5},
  pages     = {679--684},
  publisher = {Indiana University Mathematics Department},
  title     = {A Markovian Decision Process},
  urldate   = {2024-01-11},
  volume    = {6},
  year      = {1957}
}

@article{kaelbling1996reinforcement,
  title   = {Reinforcement learning: a survey},
  author  = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
  journal = {Journal of artificial intelligence research},
  volume  = {4},
  pages   = {237--285},
  year    = {1996}
}

@inproceedings{bellemare2017distributional,
  title        = {A distributional perspective on reinforcement learning},
  author       = {Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle    = {International conference on machine learning},
  pages        = {449--458},
  year         = {2017},
  organization = {PMLR}
}

% モデルフリーより優れていることの根拠
@article{deisenroth2013gaussian,
  title     = {Gaussian processes for data-efficient learning in robotics and control},
  author    = {Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  volume    = {37},
  number    = {2},
  pages     = {408--423},
  year      = {2013},
  publisher = {IEEE}
}

@inproceedings{nagabandi2018neural,
  title        = {Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author       = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle    = {2018 IEEE international conference on robotics and automation (ICRA)},
  pages        = {7559--7566},
  year         = {2018},
  organization = {IEEE}
}

% 時空間モデル、mbrlの初期の手法

@article{levine2014learning,
  title   = {Learning neural network policies with guided policy search under unknown dynamics},
  author  = {Levine, Sergey and Abbeel, Pieter},
  journal = {Advances in neural information processing systems},
  volume  = {27},
  year    = {2014}
}

% pilco

@inproceedings{deisenroth2011pilco,
  title     = {PILCO: A model-based and data-efficient approach to policy search},
  author    = {Deisenroth, Marc and Rasmussen, Carl E},
  booktitle = {Proceedings of the 28th International Conference on machine learning (ICML-11)},
  pages     = {465--472},
  year      = {2011}
}

% ダイナミクスのミスマッチによる性能劣化の例

@article{nagabandi2018learning,
  title   = {Learning to adapt in dynamic, real-world environments through meta-reinforcement learning},
  author  = {Nagabandi, Anusha and Clavera, Ignasi and Liu, Simin and Fearing, Ronald S and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  journal = {arXiv preprint arXiv:1803.11347},
  year    = {2018}
}

% CaDMの先行研究

@inproceedings{lee2020context,
  title        = {Context-aware dynamics model for generalization in model-based reinforcement learning},
  author       = {Lee, Kimin and Seo, Younggyo and Lee, Seunghyun and Lee, Honglak and Shin, Jinwoo},
  booktitle    = {International Conference on Machine Learning},
  pages        = {5757--5766},
  year         = {2020},
  organization = {PMLR}
}

% anti domain randomization

@article{ding2021not,
  title   = {Not only domain randomization: Universal policy with embedding system identification},
  author  = {Ding, Zihan},
  journal = {arXiv preprint arXiv:2109.13438},
  year    = {2021}
}

% recurrent dynamics model

@article{duan2016rl,
  title   = {Rl \textsuperscript{2}: Fast reinforcement learning via slow reinforcement learning},
  author  = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal = {arXiv preprint arXiv:1611.02779},
  year    = {2016}
}

% 世界モデルの説明

@article{ha2018world,
  title   = {World models},
  author  = {Ha, David and Schmidhuber, J{\"u}rgen},
  journal = {arXiv preprint arXiv:1803.10122},
  volume  = {2},
  number  = {3},
  year    = {2018}
}

@article{ha2018recurrent,
  title={Recurrent world models facilitate policy evolution},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

% MPCの説明
@article{kouvaritakis2016model,
  title     = {Model predictive control},
  author    = {Kouvaritakis, Basil and Cannon, Mark},
  journal   = {Switzerland: Springer International Publishing},
  volume    = {38},
  number    = {13-56},
  pages     = {7},
  year      = {2016},
  publisher = {Springer}
}

% DMControlの説明

@misc{tassa2018deepmindcontrolsuite,
  title         = {DeepMind Control Suite},
  author        = {Yuval Tassa and Yotam Doron and Alistair Muldal and Tom Erez and Yazhe Li and Diego de Las Casas and David Budden and Abbas Abdolmaleki and Josh Merel and Andrew Lefrancq and Timothy Lillicrap and Martin Riedmiller},
  year          = {2018},
  eprint        = {1801.00690},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/1801.00690}
}

@inproceedings{dey2017gate,
  title        = {Gate-variants of gated recurrent unit (GRU) neural networks},
  author       = {Dey, Rahul and Salem, Fathi M},
  booktitle    = {2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)},
  pages        = {1597--1600},
  year         = {2017},
  organization = {IEEE}
}

@article{agarwal2021deep,
  title   = {Deep reinforcement learning at the edge of the statistical precipice},
  author  = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal = {Advances in neural information processing systems},
  volume  = {34},
  pages   = {29304--29320},
  year    = {2021}
}

@misc{williams2015modelpredictivepathintegral,
  title         = {Model Predictive Path Integral Control using Covariance Variable Importance Sampling},
  author        = {Grady Williams and Andrew Aldrich and Evangelos Theodorou},
  year          = {2015},
  eprint        = {1509.01149},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SY},
  url           = {https://arxiv.org/abs/1509.01149}
}

% 序論の動機づけ

@article{yang2018grand,
  title     = {The grand challenges of science robotics},
  author    = {Yang, Guang-Zhong and Bellingham, Jim and Dupont, Pierre E and Fischer, Peer and Floridi, Luciano and Full, Robert and Jacobstein, Neil and Kumar, Vijay and McNutt, Marcia and Merrifield, Robert and others},
  journal   = {Science robotics},
  volume    = {3},
  number    = {14},
  pages     = {eaar7650},
  year      = {2018},
  publisher = {American Association for the Advancement of Science}
}

@article{tang2025deep,
  title     = {Deep reinforcement learning for robotics: A survey of real-world successes},
  author    = {Tang, Chen and Abbatematteo, Ben and Hu, Jiaheng and Chandra, Rohan and Mart{\'\i}n-Mart{\'\i}n, Roberto and Stone, Peter},
  journal   = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume    = {8},
  number    = {1},
  pages     = {153--188},
  year      = {2025},
  publisher = {Annual Reviews}
}

% day dreamerの説明

@inproceedings{wu2023daydreamer,
  title={Daydreamer: World models for physical robot learning},
  author={Wu, Philipp and Escontrela, Alejandro and Hafner, Danijar and Abbeel, Pieter and Goldberg, Ken},
  booktitle={Conference on robot learning},
  pages={2226--2240},
  year={2023},
  organization={PMLR}
}

@inproceedings{mehta2020active,
  title={Active domain randomization},
  author={Mehta, Bhairav and Diaz, Manfred and Golemo, Florian and Pal, Christopher J and Paull, Liam},
  booktitle={Conference on Robot Learning},
  pages={1162--1176},
  year={2020},
  organization={PMLR}
}