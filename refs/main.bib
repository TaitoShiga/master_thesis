% ==============================
% 序章の参考文献
% ==============================

% Atariゲームでのモデルベース強化学習の成功例
@article{hafner2020mastering,
  title   = {Mastering atari with discrete world models},
  author  = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  journal = {arXiv preprint arXiv:2010.02193},
  year    = {2020}
}

% Control系の複雑タスクの成功例
@article{hafner2019dream,
  title   = {Dream to control: Learning behaviors by latent imagination},
  author  = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal = {arXiv preprint arXiv:1912.01603},
  year    = {2019}
}

@article{lee2020stochastic,
  title   = {Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model},
  author  = {Lee, Alex X and Nagabandi, Anusha and Abbeel, Pieter and Levine, Sergey},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {741--752},
  year    = {2020}
}

% モデルフリーよりもサンプル効率がいいことの根拠

@inproceedings{NIPS2014_c7c9344b,
  author    = {Levine, Sergey and Abbeel, Pieter},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c7c9344b5a3c0533e29fa69ce807cf08-Paper.pdf},
  volume    = {27},
  year      = {2014}
}

% TDMPC2 のちの前提知識の章でも参照
@article{hansen2023td,
  title   = {Td-mpc2: Scalable, robust world models for continuous control},
  author  = {Hansen, Nicklas and Su, Hao and Wang, Xiaolong},
  journal = {arXiv preprint arXiv:2310.16828},
  year    = {2023}
}

% Domain Randomization の説明
@inproceedings{tobin2017domain,
  title        = {Domain randomization for transferring deep neural networks from simulation to the real world},
  author       = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle    = {2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)},
  pages        = {23--30},
  year         = {2017},
  organization = {IEEE}
}

% 強化学習の説明用
@book{sutton2018reinforcement,
  title     = {Reinforcement Learning, second edition: An Introduction},
  author    = {Sutton, R.S. and Barto, A.G.},
  isbn      = {9780262352703},
  series    = {Adaptive Computation and Machine Learning series},
  url       = {https://books.google.co.jp/books?id=uWV0DwAAQBAJ},
  year      = {2018},
  publisher = {MIT Press}
}

@article{markov,
  issn      = {00959057, 19435274},
  url       = {http://www.jstor.org/stable/24900506},
  author    = {R.Bellman},
  journal   = {Journal of Mathematics and Mechanics},
  number    = {5},
  pages     = {679--684},
  publisher = {Indiana University Mathematics Department},
  title     = {A Markovian Decision Process},
  urldate   = {2024-01-11},
  volume    = {6},
  year      = {1957}
}

@article{kaelbling1996reinforcement,
  title   = {Reinforcement learning: A survey},
  author  = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal = {Journal of artificial intelligence research},
  volume  = {4},
  pages   = {237--285},
  year    = {1996}
}

@inproceedings{bellemare2017distributional,
  title        = {A distributional perspective on reinforcement learning},
  author       = {Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle    = {International conference on machine learning},
  pages        = {449--458},
  year         = {2017},
  organization = {PMLR}
}

% モデルフリーより優れていることの根拠
@article{deisenroth2013gaussian,
  title     = {Gaussian processes for data-efficient learning in robotics and control},
  author    = {Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  volume    = {37},
  number    = {2},
  pages     = {408--423},
  year      = {2013},
  publisher = {IEEE}
}

@inproceedings{nagabandi2018neural,
  title        = {Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author       = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle    = {2018 IEEE international conference on robotics and automation (ICRA)},
  pages        = {7559--7566},
  year         = {2018},
  organization = {IEEE}
}

% 時空間モデル、mbrlの初期の手法

@article{levine2014learning,
  title   = {Learning neural network policies with guided policy search under unknown dynamics},
  author  = {Levine, Sergey and Abbeel, Pieter},
  journal = {Advances in neural information processing systems},
  volume  = {27},
  year    = {2014}
}

% pilco

@inproceedings{deisenroth2011pilco,
  title     = {PILCO: A model-based and data-efficient approach to policy search},
  author    = {Deisenroth, Marc and Rasmussen, Carl E},
  booktitle = {Proceedings of the 28th International Conference on machine learning (ICML-11)},
  pages     = {465--472},
  year      = {2011}
}

% ダイナミクスのミスマッチによる性能劣化の例

@article{nagabandi2018learning,
  title   = {Learning to adapt in dynamic, real-world environments through meta-reinforcement learning},
  author  = {Nagabandi, Anusha and Clavera, Ignasi and Liu, Simin and Fearing, Ronald S and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  journal = {arXiv preprint arXiv:1803.11347},
  year    = {2018}
}

% CaDMの先行研究

@inproceedings{lee2020context,
  title        = {Context-aware dynamics model for generalization in model-based reinforcement learning},
  author       = {Lee, Kimin and Seo, Younggyo and Lee, Seunghyun and Lee, Honglak and Shin, Jinwoo},
  booktitle    = {International Conference on Machine Learning},
  pages        = {5757--5766},
  year         = {2020},
  organization = {PMLR}
}

% anti domain randomization

@article{ding2021not,
  title   = {Not only domain randomization: Universal policy with embedding system identification},
  author  = {Ding, Zihan},
  journal = {arXiv preprint arXiv:2109.13438},
  year    = {2021}
}

% recurrent dynamics model

@article{duan2016rl,
  title   = {Rl \textsuperscript{2}: Fast reinforcement learning via slow reinforcement learning},
  author  = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal = {arXiv preprint arXiv:1611.02779},
  year    = {2016}
}

% 世界モデルの説明

@article{ha2018world,
  title={World models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  volume={2},
  number={3},
  year={2018}
}

% MPCの説明
@article{kouvaritakis2016model,
  title={Model predictive control},
  author={Kouvaritakis, Basil and Cannon, Mark},
  journal={Switzerland: Springer International Publishing},
  volume={38},
  number={13-56},
  pages={7},
  year={2016},
  publisher={Springer}
}