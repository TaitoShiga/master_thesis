\chapter{序論}

深層ニューラルネットワークを関数近似器として用いる強化学習（RL）は,
ボードゲームやビデオゲーム,そして複雑なロボット制御タスクにおいて,人間を凌駕する,
あるいはそれに匹敵する意思決定能力を示してきた\cite{hafner2020mastering,hafner2019dream,lee2020stochastic}.
その中でもモデルベース強化学習（MBRL）は,環境のダイナミクスを学習し,そのモデルを用いて計画（Planning）を行うことで,一般にモデルフリー手法よりも高いサンプル効率を実現する\cite{NIPS2014_c7c9344b}.
近年では,TD-MPC2に代表される潜在空間モデルを用いた手法が,高次元な観測を効率的に処理し,マルチタスク学習においても極めて高い性能を達成している.

しかし,MBRL手法には,学習時とは異なる遷移ダイナミクスを持つ未知の環境において,
性能が著しく低下するという課題がある .
実世界への適用（Sim2Real）を考えた際,ロボットの質量や摩擦,
アクチュエータの応答特性といった物理パラメータは,個体差や経年劣化,
外的要因によって常に変動し得る . 
先行研究においても,例えばCartPoleタスクにおけるポールの質量の微小な変化が,
次状態の予測精度を悪化させ,結果として計画の破綻と制御性能の劣化を招くことが示されている .
このようなダイナミクスの変化に対する脆弱性は,
強化学習エージェントを実社会へ配備する上での大きな障壁となっている .

現在のSOTA（State-of-the-Art）手法であるTD-MPC2においても,この問題は例外ではない.
TD-MPC2の世界モデルは,学習時の環境特性を内包した「静的な」モデルであり,
動作中に遭遇する未知の物理摂動に対してオンラインで自己適応する機構を十分に備えていない. 
これに対し,ダイナミクスの変化に適応する戦略として,
メタ学習やコンテキスト変数の導入が提案されてきた .
特に,環境の「コンテキスト同定」と「ダイナミクス予測」を分離するアプローチは,
環境情報の効果的な抽出に有効であるとされる . 
本研究では,この知見に基づき,TD-MPC2に明示的な物理パラメータ推定器を統合する.
GRU（Gated Recurrent Unit）を用い,過去50ステップの挙動履歴から環境の物理特性
（質量やアクチュエータの摂動等）を直接同定し,その推定値をTD-MPC2の予測モデル全体に注入する.
これにより,エージェントは未知の環境下でも即座に世界モデルを修正し,
最適な計画を維持することが可能となる.

本研究の主な貢献を以下に示す.

\begin{itemize}
  \item \textbf{適応的フレームワークの構築}  
  最先端のモデルベース強化学習手法である TD-MPC2 に対し，履歴情報に基づく明示的な物理パラメータ推定機構を導入した，新たな適応型アーキテクチャを提案した.

  \item \textbf{物理摂動に対する頑健性の実証}  
  Pendulum-Swingup および Walker-Walk などの制御タスクを対象として，物理パラメータの変化が TD-MPC2 の性能に与える影響を体系的に解析し，提案手法が非適応型ベースラインを有意に上回る性能を示すことを確認した.

  \item \textbf{解釈性の向上と課題の整理}  
  物理パラメータを明示的に推定する枠組みにより，エージェントの内部状態に対する解釈可能性を提供するとともに，推定精度と制御性能との関係を定量的に評価し，今後の課題を整理した.
\end{itemize}


本論文は以下の7章で構成される. 
第2章では,本研究の基礎となる強化学習および先行研究について概説する. 
第3章では,基盤技術であるTD-MPC2のアルゴリズムと,時系列処理技術について述べる.
第4章では,提案する物理パラメータ推定に基づく適応的TD-MPC2の詳細を記述する. 
第5章では,実験設定および評価指標について説明する. 
第6章では,実験結果に基づき,提案手法の有効性と限界について考察を行う.
第7章において,本研究の結論と今後の展望を述べる.