\chapter{考察}
\label{chap:discussion}

本章では，第\ref{chap:results}章で示した実験結果に対する解釈および考察を行う．
各手法の性能差が生じた要因を分析し，
提案手法の有効性と限界について議論する．


\section{物理パラメータ摂動に対する性能差の解釈}
\label{sec:performance_difference_interpretation}

本節では，物理パラメータが変化した環境における各手法の性能差について，
その要因を分析する．


\subsection{Pendulum-Swingup における手法間性能差}
\label{subsec:pendulum_performance_discussion}

図\ref{fig:performance_curves}および表\ref{tab:pendulum_results}に示したように，
Pendulum-Swingup タスクにおいて，
Domain Randomization（DR）は提案手法（Adaptive）を上回る性能を示した．
この結果は，タスクの特性と各手法の適応メカニズムの相互作用によって説明される．

振子の質量が増加すると，
同じ角度変化を実現するために必要なトルクが増大する．
このため，質量パラメータの変化に対する適応には，
より大きな出力を生成する制御則が求められる．
Domain Randomization では，
訓練時に質量パラメータを広範囲にわたってランダム化することで，
多様なトルク要求に対応可能なロバストな方策を獲得している．
その結果，テスト時に質量が変化した環境においても，
既に経験した範囲内であれば高い性能を維持できる．

これに対し，提案手法（Adaptive）は，
エピソード内で物理パラメータをオンライン推定し，
推定値に基づいて動力学モデルを適応させる．
図\ref{fig:pendulum_estimation_convergence}に示されるように，
推定値はエピソード初期に大きく変動し，
時間の経過とともに真値へ収束する挙動を示す．
この収束過程において，
推定値が真値から乖離している期間では，
動力学モデルの予測精度が低下し，
制御性能が一時的に劣化する．
特に，エピソード初期の推定ラグは累積報酬の低下に直接影響するため，
Pendulum のような比較的単純なタスクにおいては，
推定ラグのコストが適応の利益を上回る結果となったと考えられる．

一方，Oracle TD-MPC2 は，
すべての質量条件において最も高い性能を示している．
これは，真の物理パラメータを初めから利用できるため，
推定ラグが存在せず，
エピソード全体を通して最適な動力学モデルに基づく制御が可能であることに起因する．
Oracle の性能は，
物理パラメータが正確に既知である場合の性能上限を示しており，
提案手法における推定精度の向上が性能改善の鍵となることを示唆している．


\subsection{Walker-Walk における手法間性能差}
\label{subsec:walker_performance_discussion}

図\ref{fig:walker_actuator_performance}および表\ref{tab:walker_results}に示したように，
Walker-Walk タスクにおいては，
提案手法（Adaptive）が平均的に最も高い性能を示し，
特に低アクチュエータスケール条件において顕著な優位性が確認された．
この結果は，タスクの複雑性と適応メカニズムの有効性の関係を示唆している．

Walker-Walk タスクは，
6自由度のアクチュエータを協調的に制御する必要があり，
Pendulum-Swingup と比較して状態空間および行動空間の次元が高い．
このような高次元タスクにおいては，
Domain Randomization により獲得される方策は，
広範なパラメータ範囲に対するロバスト性を獲得する一方で，
特定のパラメータ条件に対する最適性は犠牲になる可能性がある．

これに対し，提案手法は，
エピソード内でアクチュエータスケールを推定し，
推定値に基づいて動力学モデルを調整することで，
各環境条件に特化した制御を実現できる．
特に，アクチュエータスケールが小さい条件では，
出力制約が厳しくなるため，
正確な動力学モデルに基づく最適化の重要性が増す．
提案手法は，この条件下において，
推定に基づく適応により Non-adaptive および DR を大きく上回る性能を達成している．

% また，表\ref{tab:walker_results}において，
% アクチュエータスケールが訓練条件（$1.0\times$）から離れるほど，
% 提案手法と他手法の性能差が拡大する傾向が観測される．
% これは，訓練範囲外の条件において，
% オンライン適応の利益がより顕著に現れることを示している．


\section{動的環境における適応性能の要因分析}
\label{sec:dynamic_adaptation_analysis}

図\ref{fig:walker_dynamic_compare}に示したように，
アクチュエータスケールがエピソード内で時間的に変化する動的環境において，
提案手法は他手法と比較して高い報酬を維持した．
本節では，この動的適応性能の要因を分析する．

エピソード内でアクチュエータスケールが線形に減衰する環境では，
固定された方策を用いる Non-adaptive TD-MPC2 は，
時間の経過とともに環境との不整合が拡大し，
性能が徐々に低下する．
Domain Randomization は，
訓練時に多様なスケール条件を経験しているため，
Non-adaptive よりも緩やかな性能低下を示すが，
エピソード後半では依然として劣化が観測される．

提案手法は，図\ref{fig:walker_dynamic_estimation}に示されるように，
時間とともに変化するアクチュエータスケールを追従的に推定する能力を持つ．
推定値は真値の変化に遅れながらも同様の減少傾向を示しており，
この追従能力により，
エピソード後半においても動力学モデルと実環境の乖離を抑制できる．
その結果，図\ref{fig:walker_dynamic_compare}の下段に示されるように，
提案手法は後半まで高い報酬を維持し，
他手法よりも小さい劣化に留まっている．

この結果は，提案手法が静的な環境変化だけでなく，
動的に変化する環境に対しても適応可能であることを示しており，
実世界の経年劣化や連続的な環境変動への対応において有用性が高いと考えられる．


\section{推定挙動と制御性能の関係}
\label{sec:estimation_control_relationship}

図\ref{fig:pendulum_estimation_convergence}および図\ref{fig:walker_estimation_convergence}に示されるように，
提案手法における物理パラメータの推定値は，
エピソード初期に大きく変動した後，
時間の経過とともに真値付近へ収束する挙動を示す．
本節では，この推定挙動と制御性能の関係について考察する．

推定値がエピソード初期に変動する理由として，
初期状態における観測データの不足が挙げられる．
物理パラメータ推定器は，
観測された状態遷移の履歴に基づいて推定を行うため，
エピソード開始直後は十分な情報が蓄積されておらず，
推定値は不確実性が高い状態にある．
相互作用が進行し観測データが蓄積されるにつれて，
推定値は真値に近づき，安定化する．

興味深いことに，図\ref{fig:walker_actuator_performance}および図\ref{fig:walker_dynamic_compare}に示されるように，
推定値が完全に収束していない状態においても，
提案手法は高い制御性能を維持している．
これは，TD-MPC2 のモデル予測制御が，
ある程度の動力学モデル誤差に対してロバストであることを示唆している．
モデル予測制御では，
現在の状態から短い時間窓で軌道最適化を行い，
その結果得られた行動のうち最初の一歩のみを実行する．
この再計画のサイクルにより，
モデル誤差の累積が抑制され，
推定値に多少の誤差が含まれていても，
実用的な制御性能が得られると考えられる．

ただし，推定誤差が大きい場合や，
推定値が真値から大きく乖離している期間が長い場合には，
制御性能の低下が避けられない．


\section{提案手法の有効性}
\label{sec:effectiveness}

本研究の実験結果を総合すると，
提案手法は以下の条件において有効性が高いと考えられる：

\begin{itemize}
\item \textbf{高次元・複雑なタスク}：
Walker-Walk のような高次元タスクでは，
オンライン適応の利益が推定ラグのコストを上回る．

\item \textbf{訓練範囲外の大きな摂動}：
パラメータが訓練範囲から大きく逸脱する条件では，
Domain Randomization のロバスト性では対応しきれず，
適応的な推定が有効となる．

\item \textbf{動的に変化する環境}：
エピソード内でパラメータが時間変化する場合，
固定方策では対応困難であり，
追従的な推定が必要となる．
\end{itemize}

これらの知見は，
実環境へのモデルベース強化学習の展開において，
環境特性の明示的な推定が有効な適応戦略となりうることを示している．
