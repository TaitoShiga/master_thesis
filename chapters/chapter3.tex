\chapter{前提知識}
\label{chap:preliminary}

本章では，本研究の基盤となる, 潜在空間における世界モデルと，
それを用いた制御アルゴリズムである TD-MPC2 の数理的定義について述べる．
また，環境の不確実性に対してオンラインで適応するために不可欠な，
相互作用履歴に基づく系列推論の枠組みについて整理する．


\section{潜在空間モデル予測制御と TD-MPC2}
\label{sec:tdmpc2}

本節では，本研究で用いる制御アルゴリズムの基盤となる
潜在空間世界モデルとモデル予測制御（Model Predictive Control; MPC）の枠組みを整理し，
その上で TD-MPC2 の具体的な構成について説明する．
本研究では，環境ダイナミクスの不確実性が計画性能に与える影響を議論するため，
まず計画が依存するモデル構造を明確に定義する．

\subsection{潜在空間世界モデル}
\label{subsec:latent_world_model}

世界モデル（World Model）とは，
環境の状態遷移ダイナミクスを内部モデルとして近似し，
そのモデル上で将来の状態および報酬を予測する枠組みである\cite{ha2018recurrent}．
MBRLでは，
この内部モデルを用いて将来の挙動をシミュレートし，
行動計画を行う．

ロボット制御における観測 $\mathbf{o}_t \in \mathcal{O}$ は，
画像や多次元センサ値などの高次元ベクトルであることが多い．
そこで世界モデルでは，
観測を低次元の潜在状態
$\mathbf{s}_t \in \mathbb{R}^d$
に写像し，
潜在空間上で状態遷移を記述する．

潜在空間世界モデルは，
以下の関数群によって定義される：

\begin{align}
\mathbf{s}_t &= f_{\mathrm{enc}}(\mathbf{o}_t), \\
\mathbf{s}_{t+1} &= f_{\mathrm{dyn}}(\mathbf{s}_t, \mathbf{a}_t), \\
\hat{r}_t &= f_{\mathrm{rew}}(\mathbf{s}_t), \\
\hat{q}_t &= f_{\mathrm{val}}(\mathbf{s}_t).
\end{align}

ここで，
$\mathbf{o}_t$ は時刻 $t$ における観測，
$\mathbf{a}_t \in \mathcal{A}$ は行動，
$\mathbf{s}_t$ は潜在状態を表す．
$f_{\mathrm{enc}}:\mathcal{O}\rightarrow\mathbb{R}^d$ は観測エンコーダ，
$f_{\mathrm{dyn}}:\mathbb{R}^d\times\mathcal{A}\rightarrow\mathbb{R}^d$ は潜在遷移モデル，
$f_{\mathrm{rew}}:\mathbb{R}^d\rightarrow\mathbb{R}$ は報酬予測モデル，
$f_{\mathrm{val}}:\mathbb{R}^d\rightarrow\mathbb{R}$ は価値関数近似器である．
また，
$\hat{r}_t$ および $\hat{q}_t$ は，
それぞれ予測された即時報酬および状態価値を表す．

潜在遷移モデル $f_{\mathrm{dyn}}$ は，
環境の真の遷移分布
$p(s_{t+1}\mid s_t, a_t)$
を近似する写像として学習される．
このモデルを反復的に適用することで，
将来の潜在状態列
$\{\mathbf{s}_{t+1}, \mathbf{s}_{t+2}, \ldots\}$
を予測することができる．

\subsection{潜在空間上のモデル予測制御}
\label{subsec:latent_mpc}

MPCは，
学習された遷移モデルを用いて，
現在時刻から有限ホライゾン $H$ ステップ先までの行動系列を最適化する制御手法である\cite{kouvaritakis2016model}．
TD-MPC2 では，
潜在空間上で定義された遷移モデルを用いて行動系列の最適化を行う．

時刻 $t$ において，
現在の潜在状態 $\mathbf{s}_t$ が与えられたとき，
MPC は次の最適化問題を解く：

\begin{align}
\mathbf{a}^*_{t:t+H-1}
=
\arg\max_{\mathbf{a}_{t:t+H-1}}
\mathbb{E}
\left[
\sum_{k=0}^{H-1} \gamma^k \hat{r}_{t+k}
+ \gamma^H \hat{q}(\mathbf{s}_{t+H})
\right].
\end{align}

ここで，
$\hat{r}_{t+k}$ および $\hat{q}(\mathbf{s}_{t+H})$ は，
それぞれ学習された報酬モデルおよび価値関数によって予測される報酬,行動価値である．
潜在状態の推移は，
初期状態 $\mathbf{s}_t$ から始めて，
遷移モデル $f_{\mathrm{dyn}}$ を反復適用することで計算される：

\begin{align}
\mathbf{s}_{t+k+1} = f_{\mathrm{dyn}}(\mathbf{s}_{t+k}, \mathbf{a}_{t+k}),
\quad k = 0, \dots, H-1.
\end{align}

このようにして得られた潜在軌道
$\{\mathbf{s}_t, \mathbf{s}_{t+1}, \dots, \mathbf{s}_{t+H}\}$
に基づき，
各候補行動系列の期待リターンを評価する．
\subsection{TD-MPC2 の構成}
\label{subsec:tdmpc2_architecture}

TD-MPC2 は，
潜在空間世界モデルに基づくモデル予測制御と，
TD 学習による価値関数推定を統合したMBRL手法であり,
以下の 5 つのニューラルネットワークから構成される\cite{hansen2023td}：

\begin{itemize}
\item \textbf{Encoder $f_{\mathrm{enc}}$}：
観測 $\mathbf{o}_t \in \mathcal{O}$ を
潜在状態 $\mathbf{s}_t \in \mathbb{R}^d$ に写像する．
\item \textbf{Latent Dynamics $f_{\mathrm{dyn}}$}：
潜在状態 $\mathbf{s}_t$ と行動 $\mathbf{a}_t \in \mathcal{A}$ を入力とし，
次の潜在状態 $\mathbf{s}_{t+1}$ を予測する．
\item \textbf{Reward Model $f_{\mathrm{rew}}$}：
潜在状態 $\mathbf{s}_t$ における即時報酬
$\hat{r}_t \in \mathbb{R}$ を予測する．
\item \textbf{Value Model $f_{\mathrm{val}}$}：
潜在状態 $\mathbf{s}_t$ における将来価値
$\hat{q}_t \in \mathbb{R}$ を予測する．
\item \textbf{Policy Prior $\pi_{\mathrm{prior}}$}：
MPC における行動サンプリングの初期分布を与える．
\end{itemize}

これらのモデルは，
観測系列 $\{\mathbf{o}_t, \mathbf{o}_{t+1}, \ldots\}$, 
行動系列 $\{\mathbf{a}_t, \mathbf{a}_{t+1}, \ldots\}$, および報酬系列 $\{r_t, r_{t+1}, \ldots\}$  から構成される
遷移データを用いて, 同時に学習される.

具体的には，
潜在遷移モデル $f_{\mathrm{dyn}}$ を $K$ ステップ反復適用することで得られる
潜在状態列
$\mathbf{s}_{t+1:t+K}$
に対し，
報酬予測誤差および価値予測誤差を最小化するよう学習が行われる．

行動選択時には，
\ref{subsec:latent_mpc}節で定義した MPC が用いられ，
潜在遷移モデル・報酬モデル・価値モデルを用いて
行動系列の最適化が行われる．

\section{相互作用履歴からの系列推論}
\label{sec:sequential_inference}

標準的なRLアルゴリズムの多くは，
環境のマルコフ性を仮定し，
現在の観測のみに基づいて行動を決定する．
しかし，物理パラメータが未知であるような不確実環境下では，
単一時刻の観測情報だけでは環境の状態を完全に同定することは困難である．
本節では，
このような環境を部分観測マルコフ決定過程として定式化し，
過去の履歴から環境特性を推論する手法について述べる．

\subsection{部分観測環境と環境特性の定式化}
\label{subsec:context_importance}

質量や摩擦係数などの物理パラメータ $\theta$ は，
環境の遷移ダイナミクスを決定する重要な要素であるが，
多くの場合，観測 $\mathbf{o}_t$ からは直接得られず，
状態遷移の結果として現れる挙動の変化を通じて間接的に反映される．

このとき，環境の真の状態は，
可観測な状態 $s_t$ と未知の物理パラメータ $\theta$ を含む拡張状態
\(
x_t = (s_t, \theta)
\)
として定義される．
観測 $\mathbf{o}_t$ が $s_t$ のみの関数であるとき，
現在の観測のみを用いた意思決定はマルコフ性を失うため，
環境は部分観測マルコフ決定過程
（Partially Observable Markov Decision Process; POMDP）
として扱われる．

POMDP 下において最適な行動を選択するためには，
現在の $\mathbf{o}_t$ に加え，
過去の相互作用の経緯から
パラメータ $\theta$ を含む潜在的な環境特性を推定することが不可欠となる．

\subsection{履歴情報に基づく環境特性抽出}
\label{subsec:gru}

時刻 $t$ における過去 $K$ ステップの観測および行動の系列を，
相互作用履歴
\begin{equation}
\mathcal{H}_t =
\{(\mathbf{o}_{t-K}, \mathbf{a}_{t-K}), \ldots, (\mathbf{o}_{t-1}, \mathbf{a}_{t-1})\}
\end{equation}
として定義する．

この履歴 $\mathcal{H}_t$ には，
「特定の行動 $\mathbf{a}$ に対してどのような遷移が起きたか」
という情報が蓄積されており，
未知の環境特性を同定するための情報情報が含まれると考えられる．

履歴から環境特性を抽出する写像を
\(
\mathbf{c}_t = g(\mathcal{H}_t)
\)
と表すと，
関数 $g$ には可変長かつ時系列的な依存関係を
適切に処理する能力が求められる．
このような系列推論を実現する代表的なネットワークとして，
再帰型ニューラルネットワーク
（Recurrent Neural Network; RNN）が広く用いられる．

本研究では，
長期的依存関係の学習に長け，
かつ計算効率に優れる
Gated Recurrent Unit（GRU）を採用して
環境特性 $\mathbf{c}_t$ を抽出する\cite{dey2017gate}．
