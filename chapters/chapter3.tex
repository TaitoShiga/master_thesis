\chapter{前提知識}
\label{chap:preliminary}

本章では，本研究の基盤となる, 潜在空間における世界モデルと，
それを用いた制御アルゴリズムである TD-MPC2 の数理的定義について述べる．
また，環境の不確実性に対してオンラインで適応するために不可欠な，
相互作用履歴に基づく系列推論の枠組みについて整理する．


\section{潜在空間モデル予測制御と TD-MPC2}
\label{sec:tdmpc2}

本節では，本研究で用いる制御アルゴリズムの基盤となる
潜在空間世界モデルとモデル予測制御（Model Predictive Control; MPC）の枠組みを整理し，
その上で TD-MPC2 の具体的な構成について説明する．
本研究では，環境ダイナミクスの不確実性が計画性能に与える影響を議論するため，
まず「どのような構造の上で計画が行われているのか」を明確に定義する．

\subsection{潜在空間世界モデル}
\label{subsec:latent_world_model}

世界モデル（World Model）とは，
環境の遷移ダイナミクスを内部モデルとして学習し，
そのモデル上で将来の状態や報酬を予測する枠組みである．
ロボット制御においては，
高次元の観測（画像や多様なセンサ値）を直接用いた計画は計算的に困難であるため，
観測を低次元の潜在状態に写像し，
この潜在空間上でダイナミクスを記述する手法が一般的に採用されている．

潜在空間世界モデルは，構造的には以下の要素から構成される：

\begin{itemize}
\item 観測 $\mathbf{o}_t$ を潜在状態 $\mathbf{s}_t$ に写像するエンコーダ
\item 潜在状態と行動に基づき次状態を予測する遷移モデル
\item 潜在状態における報酬や価値を評価する予測モデル
\end{itemize}

これらは必ずしも観測の再構成を目的とする必要はなく，
制御に必要な情報のみを潜在表現に保持する設計が近年では主流である．
このような制御指向の世界モデルでは，
潜在状態 $\mathbf{s}_t$ は「将来の報酬を予測するために十分な状態」として学習され，
観測空間の忠実な復元は行わない．

このとき，潜在状態は環境の力学的性質（質量・摩擦・関節特性など）を
暗黙的に含んだ表現となる．
したがって，世界モデルに基づく計画は，
この潜在状態が環境の真のダイナミクスをどの程度正確に反映しているかに強く依存する．

\subsection{潜在空間上のモデル予測制御}
\label{subsec:latent_mpc}

モデル予測制御（MPC）は，
学習された世界モデルを用いて，
現在時刻から有限ホライゾン $H$ 先までの行動系列を最適化する手法である．
時刻 $t$ において，MPC は次の最適化問題を解く：

\begin{align}
\mathbf{a}^*_{t:t+H-1}
=
\arg\max_{\mathbf{a}_{t:t+H-1}}
\mathbb{E}
\left[
\sum_{k=0}^{H-1} \gamma^k r_{t+k}
+ \gamma^H Q(\mathbf{s}_{t+H})
\right],
\end{align}

ここで，$\mathbf{s}_{t+k}$ は潜在遷移モデルによって予測された潜在状態であり，
$Q(\cdot)$ は終端状態以降のリターンを近似する価値関数である．
このブートストラップにより，
短いホライゾンであっても長期的な影響を考慮した計画が可能となる．

重要なのは，
MPC による計画は複数ステップにわたる潜在遷移予測の積み重ねに基づくため，
モデル誤差が存在するとその影響が時間とともに累積する点である．
特に，潜在状態が実際の環境ダイナミクスと乖離している場合，
最適化された行動系列は実環境において期待通りの挙動を示さず，
制御性能の急激な劣化を引き起こす可能性がある．

\subsection{TD-MPC2 の構成}
\label{subsec:tdmpc2_architecture}

TD-MPC2 は，
潜在空間世界モデルと MPC による計画を，
TD 学習による価値推定と統合したモデルベース強化学習手法である\cite{hansen2023td}．
TD-MPC2 は，以下の 5 つのモジュールから構成される：

\begin{itemize}
\item \textbf{Encoder}：
観測 $\mathbf{o}_t$ を潜在状態 $\mathbf{s}_t$ に写像する
\item \textbf{Latent Dynamics}：
潜在状態と行動から次状態 $\mathbf{s}_{t+1}$ を予測する
\item \textbf{Reward Model}：
潜在状態における即時報酬を予測する
\item \textbf{Value Model}：
潜在状態における将来の累積報酬を推定する
\item \textbf{Policy Prior}：
MPC における探索を効率化するための事前分布
\end{itemize}

これらのモデルは，
多段階の潜在遷移ロールアウトに基づく損失関数を最小化することで同時に学習される．
TD-MPC2 は，
潜在空間上での高精度な計画と価値ブートストラップを組み合わせることで，
連続値制御タスクにおいて高い性能を示すことが知られている．

一方で，TD-MPC2 における世界モデルは，
学習時に得られた環境ダイナミクスを固定的に内部表現として保持する．
そのため，運用中に物理パラメータが変動した場合，
潜在状態が実際の環境を正しく表現できなくなり，
計画精度が著しく低下する可能性がある．
この点が，本研究で扱う環境適応の課題に直結する．



\section{相互作用履歴からの系列推論}
\label{sec:sequential_inference}

標準的な強化学習では，
現在の観測のみを用いて行動を決定することが多い．
しかし，
物理パラメータが未知であるような不確実環境では，
単一時刻の観測だけでは環境の状態を十分に同定できない．
このような状況に対処するため，
過去の相互作用履歴を考慮した系列推論が重要となる．


\subsection{不確実環境と部分観測}
\label{subsec:context_importance}

質量や摩擦係数といった物理パラメータは，
直接観測することができない一方で，
行動の結果として生じる挙動の差として時間的に現れる．
このように，
真の状態が完全には観測できない環境は，
部分観測マルコフ決定過程（Partially Observable Markov Decision Process, POMDP）
として定式化される．
POMDP においては，
過去の観測と行動の履歴を考慮した推論が不可欠となる．


\subsection{相互作用履歴の活用と GRU}
\label{subsec:gru}

過去 \(K\) ステップにわたる観測と行動の系列を，
\(\mathcal{H}_t = \{(\mathbf{o}_{t-K}, \mathbf{a}_{t-K}), \ldots, (\mathbf{o}_{t-1}, \mathbf{a}_{t-1})\}\)
と定義する．
この履歴情報から現在の環境特性を抽出するため，
本研究では再帰型ニューラルネットワーク（RNN）の一種である
Gated Recurrent Unit (GRU) を採用する．
GRU は，
内部状態をゲート機構によって制御することで，
長期的な依存関係を効率的に保持できる特徴を持つ．
これにより，
過去の相互作用に内在する挙動の違いを蓄積し，
未知の物理パラメータに関する情報を表現することが可能となる．
