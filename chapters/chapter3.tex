\chapter{前提知識}
\label{chap:preliminary}

本章では，本研究の基盤となる潜在空間における世界モデルと，
それを用いた制御アルゴリズムである TD-MPC2 の数理的定義について述べる．
また，環境の不確実性に対してオンラインで適応するために不可欠な，
相互作用履歴に基づく系列推論の枠組みについて整理する．


\section{潜在空間モデル予測制御：TD-MPC2}
\label{sec:tdmpc2}

TD-MPC2 は，
学習された世界モデル上での計画（Planning）と，TD学習による価値推定を統合した，
MBRLの手法である\cite{hansen2023td}．
本節では，
TD-MPC2 の背景にある世界モデルの概念とその変遷を整理した後，
モデル予測制御の枠組み，
および TD-MPC2 の具体的なアーキテクチャについて順に説明する．


\subsection{世界モデルの概念と変遷}
\label{subsec:world_model}

世界モデル（World Model）とは，
エージェントが環境の振る舞いを模倣した「内部シミュレータ」を学習し，
その内部モデルを用いて将来の状態や報酬を予測する枠組みを指す\cite{ha2018world}．
この概念は，
人間が行動を起こす前に「もしこの行動を取ったら何が起こるか」を頭の中で想像する，
いわゆるメンタルモデルの考え方に着想を得ている．

\paragraph{高次元観測の圧縮}

ロボットが取得する画像やセンサデータなどの観測 \(\mathbf{o}_t\) は高次元であり，
そのままの空間でダイナミクスを学習し，将来を予測することは，
計算コストおよびデータ効率の観点から困難である．
そこで世界モデルでは，
観測 \(\mathbf{o}_t\) を低次元のベクトルである潜在表現
\(\mathbf{z}_t\) に写像し，
以降の予測や計画をこの潜在空間上で行う．
この圧縮により，
環境の本質的な状態のみを抽出し，
効率的な将来予測が可能となる．

\paragraph{再構成型世界モデル}

初期の世界モデル研究では，
潜在表現 \(\mathbf{z}_t\) が観測の情報を十分に保持していることを保証するため，
Variational Autoencoder (VAE) を用いた再構成型の学習が広く用いられていた\cite{ha2018world}．
VAE では，
観測 \(\mathbf{o}_t\) から潜在表現 \(\mathbf{z}_t\) を推定するエンコーダと，
\(\mathbf{z}_t\) から元の観測を復元するデコーダを同時に学習する．
このとき，
潜在表現は「元の観測を再現できること」を目的として最適化される．

この設計により，
潜在表現は観測空間の情報を網羅的に保持するが，
背景の模様や照明条件など，
制御に直接関係しない視覚情報も含めて学習することになる．
その結果，
モデルの表現能力や学習資源が，
制御に不要な要素にも割かれる可能性がある．

\paragraph{制御中心型（陰的）世界モデル}

これに対し，
近年の制御指向の世界モデルでは，
観測の再構成を明示的な目的としない設計が採用されている．
本研究のベースラインである TD-MPC2 も，
デコーダを持たない陰的世界モデル（Implicit World Model）を採用している．
このアプローチでは，
将来の報酬や価値を正確に予測するために必要な情報のみを潜在空間に保持し，
観測の忠実な復元は行わない．

具体的には，
観測 \(\mathbf{o}_t\) はエンコーダによって潜在状態 \(\mathbf{s}_t\) に変換され，
潜在状態と行動 \(\mathbf{a}_t\) に基づいて，
次の潜在状態 \(\mathbf{s}_{t+1}\) が予測される．
この潜在遷移を繰り返すことで，
将来の軌道を潜在空間上でロールアウトし，
それに対応する報酬および価値を評価することが可能となる．

\subsection{モデル予測制御 (Model Predictive Control, MPC)}
\label{subsec:mpc}

モデル予測制御（Model Predictive Control, MPC）は，
学習した内部モデルを用いて，
現在から \(H\) ステップ先までの行動系列を計画する制御手法である\cite{kouvaritakis2016model}．
時刻 \(t\) において，
MPC は次の期待累積報酬を最大化する行動系列
\(\mathbf{a}^*_{t:t+H-1}\) を求める．

\begin{align}
\mathbf{a}^*_{t:t+H-1}
=
\arg\max_{\mathbf{a}_{t:t+H-1}}
\mathbb{E}
\left[
\sum_{k=0}^{H-1} \gamma^k r_{t+k}
+
\gamma^H Q(\mathbf{s}_{t+H}, \mathbf{a}_{t+H})
\right].
\end{align}

ここで，
TD-MPC2 の特徴は，
有限の予測ホライゾン \(H\) の終端において，
学習済みの価値関数 \(Q\) を用いて，
それ以降の長期的なリターンを近似的に評価する点にある．
このブートストラップにより，
短い予測ホライゾンであっても，
長期的な影響を考慮した計画が可能となる．


\subsection{TD-MPC2 の具体的構成}
\label{subsec:tdmpc2_architecture}

TD-MPC2 は，
以下の 5 つのモジュールから構成される．

\begin{itemize}
\item \textbf{Encoder}：
観測 \(\mathbf{o}_t\) を潜在状態 \(\mathbf{s}_t\) に写像する．
\item \textbf{Latent Dynamics}：
潜在状態 \(\mathbf{s}_t\) と行動 \(\mathbf{a}_t\) から，
次の潜在状態 \(\mathbf{s}_{t+1}\) を予測する．
\item \textbf{Reward Model}：
潜在状態において得られる即時報酬 \(\hat{r}_t\) を予測する．
\item \textbf{Value Model}：
将来得られる累積報酬の期待値 \(\hat{q}_t\) を予測する．
\item \textbf{Policy Prior}：
MPC における探索を効率化するため，
有望な行動分布を事前に与える．
\end{itemize}

これらのモデルは，
多段階の潜在遷移予測に基づく損失関数を最小化することで，
同時に学習される．
さらに，
潜在表現のスケールを安定させる \textbf{SimNorm} や，
報酬および価値予測を分類問題として扱う
\textbf{離散回帰（Discrete Regression）} といった工夫により，
学習の安定性と実用性が向上している．


\section{相互作用履歴からの系列推論}
\label{sec:sequential_inference}

標準的な強化学習では，
現在の観測のみを用いて行動を決定することが多い．
しかし，
物理パラメータが未知であるような不確実環境では，
単一時刻の観測だけでは環境の状態を十分に同定できない．
このような状況に対処するため，
過去の相互作用履歴を考慮した系列推論が重要となる．


\subsection{不確実環境と部分観測}
\label{subsec:context_importance}

質量や摩擦係数といった物理パラメータは，
直接観測することができない一方で，
行動の結果として生じる挙動の差として時間的に現れる．
このように，
真の状態が完全には観測できない環境は，
部分観測マルコフ決定過程（Partially Observable Markov Decision Process, POMDP）
として定式化される．
POMDP においては，
過去の観測と行動の履歴を考慮した推論が不可欠となる．


\subsection{相互作用履歴の活用と GRU}
\label{subsec:gru}

過去 \(K\) ステップにわたる観測と行動の系列を，
\(\mathcal{H}_t = \{(\mathbf{o}_{t-K}, \mathbf{a}_{t-K}), \ldots, (\mathbf{o}_{t-1}, \mathbf{a}_{t-1})\}\)
と定義する．
この履歴情報から現在の環境特性を抽出するため，
本研究では再帰型ニューラルネットワーク（RNN）の一種である
Gated Recurrent Unit (GRU) を採用する．
GRU は，
内部状態をゲート機構によって制御することで，
長期的な依存関係を効率的に保持できる特徴を持つ．
これにより，
過去の相互作用に内在する挙動の違いを蓄積し，
未知の物理パラメータに関する情報を表現することが可能となる．
