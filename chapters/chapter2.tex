\chapter{研究背景}
\label{chap:background}

本章では，本研究の背景として，RLおよびMBRLの基本概念と，
実環境適用において顕在化する課題を整理する．
まず\ref{chap:rl}節でRLの基礎を概説する．
続いて\ref{sec:mbrl}節で，MBRLの枠組みと近年の進展を概観する．
その上で，\ref{sec:uncertainty_mbrl}節では，学習モデルに基づく計画という性質が，
環境変動下でどのような不確実性として現れるかを整理する．
さらに\ref{sec:existing_strategies}節で，不確実性に対する既存の戦略を概観し，
最後に\ref{sec:our_position}節で本研究の立場を明確にする．

\section{強化学習}
\label{chap:rl}

\subsection{基礎理論}
\label{chap:fund_rl}

RLは，
エージェントが環境との相互作用を通じてタスクを学習することを目的とした機械学習手法である\cite{sutton2018reinforcement}．
このプロセスでは，エージェントは状態から行動を選択し，
状態遷移確率に従って次の状態へ遷移する．
各遷移に対して報酬が与えられ，
エージェントは累積報酬の最大化を目的とする最適化問題を解くことで，
望ましい行動系列を学習する．
重要な点は，エージェントが明示的な指令を受けることなく，
試行錯誤を通じて環境と相互作用することによってのみ学習が進むことである．
この過程では，即時報酬だけでなく，将来得られる報酬の予測も考慮して行動を選択する必要がある．

RLで扱われる問題は一般に，
マルコフ決定過程（Markov Decision Process: MDP）\cite{markov}として定式化される．
MDPは$\mathcal{M} = (\mu_0, \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$というタプルで表される．
ここで，$\mu_0$は初期状態分布，
$\mathcal{S}$は状態空間，
$\mathcal{A}$は行動空間を表す．
$\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}$は遷移確率であり，
$\mathcal{P}(s'|s,a)$は状態$s$において行動$a$をとったときに$s'$へ遷移する確率を表す．
$\mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}$は報酬関数である．
$\gamma\in(0,1]$は割引率であり，将来報酬の重要度を調整する．

エピソード終了条件を持たない継続タスク（non-episodic task）の場合，
累積報酬が有限に定義されるためには割引率が $\gamma<1$ を満たす必要がある．
これは，\\$\sum_{t=0}^{\infty}\gamma^t r_t$が収束するための十分条件であり，
割引率は数学的にwell-definedな価値関数を保証する役割を持つ．

エージェントは確率的方策
$\pi(a|s)$に従って行動し，
状態価値関数は
\begin{equation}
V_{\mathcal{M}}^{\pi}(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}r_t \mid s_0=s \right]
\label{eq:V}
\end{equation}
として定義される．
これは状態$s$から方策$\pi$に従って行動したときに得られる
割引累積報酬の期待値である．

同様に行動価値関数は
\begin{equation}
Q_{\mathcal{M}}^{\pi}(s,a) = \mathbb{E}_{s'\sim\mathcal{P}(\cdot|s,a)} \left[ \mathcal{R}(s,a,s')+\gamma V_{\mathcal{M}}^{\pi}(s') \right]
\label{eq:Q}
\end{equation}
として定義される．

RLの最終目標は，
最適行動価値関数$Q_{\mathcal{M}}^{*}(s,a) =\sup_{\pi} Q_{\mathcal{M}}^{\pi}(s,a)$に基づき，
最適方策$\pi_{\mathcal{M}}^{*}(s) =\argmax_{a\in\mathcal{A}} Q_{\mathcal{M}}^{*}(s,a)$を求めることである．

この目的は，
方策$\pi$に誘導される定常状態行動分布
$\mu^{\pi}(s,a)$を用いて，
以下の目的関数として表現できる：
\begin{equation}
\mathcal{J}(\pi)
= \mathbb{E}_{(s,a)\sim\mu^{\pi}}
\left[
\mathcal{R}(s,a,s')
\right]
\label{eq:J}
\end{equation}
ここで$\mu^{\pi}$は，
遷移確率$\mathcal{P}$と方策$\pi$により定まるマルコフ連鎖の定常分布であり，
$\mu^{\pi}(s,a) = d^{\pi}(s)\pi(a|s)$と表される．
$d^{\pi}(s)$は方策$\pi$に従ったときの状態定常分布である．

このとき，
目的関数$\mathcal{J}(\pi)$は
価値関数の初期状態分布に関する期待値としても表せる：
\begin{equation}
\mathcal{J}(\pi)
= \mathbb{E}_{s_0\sim\mu_0}
\left[
V_{\mathcal{M}}^{\pi}(s_0)
\right]
\label{eq:J2}
\end{equation}
すなわち，
価値関数の最大化と
目的関数$\mathcal{J}(\pi)$の最大化は等価である．

RLアルゴリズムは，
(1) 現在の方策の価値を評価する予測（policy evaluation）と，
(2) 価値に基づいて方策を改善する方策改善（policy improvement）
の二段階を反復することで，
方策最適化を行う．

\subsection{TD学習}
\label{sec:tderror}
TD学習は，環境の遷移確率や報酬関数といったMDPの完全なモデルを既知としないアルゴリズムである\cite{kaelbling1996reinforcement}．
エージェントは環境の動態を事前に与えられないため，実際に環境内で行動して得られた状態遷移や報酬の時系列データ，
すなわちMDPの実行サンプルを観測することで状態価値を推定する．
状態価値関数の更新は，以下の式により定義される．
\begin{equation}
    V_{\mathcal{M}}^{\pi}(s_t) \leftarrow V_{\mathcal{M}}^{\pi}(s_t) + \alpha \left( r_{t+1} + \gamma V_{\mathcal{M}}^{\pi}(s_{t+1}) - V_{\mathcal{M}}^{\pi}(s_t) \right)
    \label{eq:td}
\end{equation}
ここで $\alpha$ は学習率，$r_{t+1}$ は時刻 $t$ における行動の結果として，
次状態 $s_{t+1}$ へ遷移する際に得られた即時報酬である．
式中の括弧内は2つの要素の差分で構成されており，
$r_{t+1} + \gamma V_{\mathcal{M}}^{\pi}(s_{t+1})$ はTD目標（TD target）と呼ばれる．
これは観測された即時報酬と次状態の推定価値を組み合わせた値であり，
1ステップ先の遷移に基づいて得られた新たな状態価値の推定値を表す．
一方，$V_{\mathcal{M}}^{\pi}(s_t)$ は，
前回の更新までに保持されていた現在の状態 $s_t$ における価値推定値である．
これらの差はTD誤差と呼ばれ，
実際の遷移から得られた新しい見積もりと従来の予測との乖離を表している．
TD学習ではこの誤差に比例して価値関数を更新することで，
状態価値の推定精度を逐次的に改善する．
さらに，TD学習は次状態の推定値を用いて更新を行うため，
エピソードの終了を待つことなく各ステップの遷移ごとに価値推定が可能となり，
オンラインで効率的な学習を実現している．

以上のようなRLの基礎理論に対し，
実環境での応用においては大量のデータ取得に安全性やコストの制約が大きいため，
限られたデータから効率的に学習する枠組みが必要となる．
この観点から，環境モデルを明示的に学習し計画に利用するMBRLが検討されてきた．

% モデルベース強化学習の進展
% TODO: dreamerとかも引用して、tdmpc2に偏った議論を避けるようにする
\section{モデルベース強化学習}
\label{sec:mbrl}

MBRLは，
環境の遷移ダイナミクス $p(s_{t+1} \mid s_t,  a_t)$ を近似する学習モデル
$\hat{f}_{\theta}$ を構築し，これを利用して行動を決定する手法である\cite{kaelbling1996reinforcement}．
MBRLの最大の特徴は，学習済みモデル上で将来の状態遷移を予測し，
その予測に基づいた計画（Planning）が可能である点にある．
一般にMBRLは，経験を直接方策に反映させるモデルフリー手法と比較して
サンプル複雑性の面で優れており \cite{nagabandi2018neural, NIPS2014_c7c9344b, deisenroth2013gaussian}，
実機実験のコストや安全性が懸念されるロボティクス分野において
極めて重要な役割を果たす．

初期のMBRLにおいては，
ガウス過程（Gaussian Processes）\cite{deisenroth2011pilco} や
時間変化線形モデル \cite{levine2014learning}
といった比較的単純な関数近似器を用いたダイナミクス学習が主流であった．
特に PILCO \cite{deisenroth2011pilco} は，
確率的なダイナミクスモデルとモデルの不確実性を
長期的な計画に組み込むことで，
優れたサンプル効率を達成した．
しかし，これらの古典的な近似手法は，
高次元の状態空間や摩擦接触を含む複雑な非線形ダイナミクスを
正確に捉えることが困難であるという課題を有していた
\cite{nagabandi2018neural}．

これに対し，近年では深層ニューラルネットワーク
（Deep Neural Networks; DNN）を
高容量な関数近似器として導入することで，
高次元かつ複雑なダイナミクスを持つ
ロボット制御タスクへの適用が可能となっている．

DNNを用いたモデル学習においては，
高次元の観測値を直接予測する代わりに，
制御に必要な情報を凝縮した低次元の潜在空間
（Latent Space）上でダイナミクスを記述する手法が
主流となっている．
これにより，高次元観測に伴う計算コストの抑制と，
長期的なホライゾンにおける予測精度の維持を
両立している．

エージェントは各時刻 $t$ において，
学習されたモデルを用いて以下の最適化問題を解くことで
行動系列を決定する．
\begin{equation}
(a_t,  \dots,  a_{t+H-1})
=
\argmax_{a_t,  \dots,  a_{t+H-1}}
\sum_{t'=t}^{t+H-1}
\gamma^{t'-t}
r(s_{t'},  a_{t'})
\label{eq:mbrl_planning}
\end{equation}
ここで $H$ は予測ホライゾンを表す\cite{nagabandi2018neural}．
$s_{t'}$ は世界モデルによって予測される潜在状態である.
また $r(s_{t'}, a_{t'})$ は，
潜在状態 $s_{t'}$ と行動 $a_{t'}$ に基づいて
報酬モデルによって予測される報酬を表す.

この枠組みの代表例である TD-MPC2 \cite{hansen2023td} は，
潜在空間上での状態遷移，報酬予測，
および価値関数の学習を統合し，
サンプリングベースの最適化手法を用いることで，
複雑な連続値制御タスクにおいて高い性能を示している一方で，
内部に保持する世界モデルは
学習時の環境パラメータに固執した
静的な表現になりやすく，
運用中に生じる物理的な摂動への適応には
課題を残している．

% モデルベース強化学習における環境の不確実性

\section{モデルベース強化学習における環境不確実性}
\label{sec:uncertainty_mbrl}

MBRLは，\ref{sec:mbrl}節で述べたように,  学習されたダイナミクスモデルに基づいて
将来の状態遷移を予測し，その予測結果を用いて計画を行う枠組みである．
このため，モデルの予測精度は制御性能に直接的かつ決定的な影響を及ぼす．
特に，複数ステップ先の状態遷移を考慮する計画型手法においては，
わずかな予測誤差であっても，その影響が時間とともに累積し，
行動選択の妥当性を大きく損なう可能性がある．

しかし，実環境への適用を考えた場合，
学習時に想定された環境と完全に同一の遷移ダイナミクスが
運用中も維持されるとは限らない．
ロボットの個体差，部品の摩耗や経年劣化，
あるいは接触条件や外乱といった環境側の要因により，
実際の遷移ダイナミクスは学習時とは異なるものとなり得る．

このようなダイナミクスのミスマッチが生じた場合，
学習済みモデルに基づく状態予測は徐々に現実から乖離していく．
とりわけ，モデル予測制御（Model Predictive Control; MPC）に代表される
計画型アプローチでは，
予測誤差が行動計画全体に波及し，
意図しない状態遷移やタスク失敗を引き起こす要因となる．
この問題は，接触を含む複雑なタスクや，
長い予測ホライゾンを必要とするタスクにおいて
より顕著に現れることが知られている\cite{nagabandi2018learning, lee2020context}．

近年の高性能なMBRL手法においても，
この不確実性の問題は本質的には未解決であり，学習過程を通じて獲得された
単一の環境ダイナミクスを内部モデルとして保持し，
それを固定的に用いた計画が行われる．
その結果，運用中に生じる物理パラメータの変動や
環境条件の変化に対して，
モデルを動的に修正・適応する機構を十分に備えていない場合が多い．

このように，現在のMBRL手法は，
環境が不変であるという暗黙の前提の下で設計されており，
パラメータが継続的に変動し得る実世界環境においては，
適応能力の不足が実用上の大きな課題となっている\cite{lee2020context,wu2023daydreamer}．  

% 不確実性に対する既存の戦略

\section{環境不確実性に対する既存の戦略}
\label{sec:existing_strategies}

前節で述べたように，MBRLは，
学習されたダイナミクスモデルに基づいて計画を行うため，
環境ダイナミクスの変動に起因する予測誤差が
制御性能に直接的な影響を及ぼす．
このようなダイナミクスのミスマッチに対処するため，
これまでの研究では大きく分けて
\emph{頑健化（Robustness）}と
\emph{適応化（Adaptation）}
という二つの戦略が検討されてきた．

頑健化の代表的な手法として，
Domain Randomization（DR）が広く用いられている\cite{tobin2017domain}．
DR では，シミュレーション上での学習段階においてリンクの質量や摩擦係数などの
物理パラメータを一定の範囲内でランダムに変化させることで，
単一の方策が多様な環境条件下でも
平均的に良好な性能を発揮することを目指す．
このアプローチは実装が比較的容易であり，
Sim2Realの文脈においても一定の成功を収めてきた\cite{tobin2017domain, mehta2020active}．

一方で，DR はあらゆる環境変動に対して
単一の方策で対応することを前提とするため，
特定の環境条件における最適性を犠牲にした
保守的な制御になりやすいという課題を有する．
また，想定された分布の外側にある変動や，
時間的に変化するパラメータに対しては，
十分な性能を維持できない場合も少なくない\cite{ding2021not}．

これに対し，実行時に環境の変化を検知し，
モデルや方策を動的に調整する
適応化のアプローチが提案されてきた．
この系譜には，過去の遷移履歴を用いて
モデルパラメータを更新するメタ学習的手法\cite{nagabandi2018learning}や，
リカレント構造の内部状態を更新することで
環境変化に追従する手法が含まれる\cite{duan2016rl}．

近年では，環境の特性を
「コンテキスト」と呼ばれる潜在変数として抽出し，
それを条件付け情報として
ダイナミクスモデルや方策に入力する
コンテキスト学習の枠組みが注目を集めている\cite{lee2020context}．
このような手法では，
未知の環境要因や複合的な変動を
潜在表現に吸収することで，
幅広い環境条件への適応が可能となる．
その結果，頑健化手法と比較して，
特定の環境に特化した鋭い制御を実現できる点が
大きな利点とされている．

しかし，これらの既存手法において，
抽出されるコンテキストは
物理的な意味を持たない潜在表現であり，
何が原因でダイナミクスが変化したのかを
明確に解釈することは困難である．
そのため，性能劣化が生じた際の
失敗要因の特定や，
実運用における信頼性評価・デバッグを
体系的に行うことが難しいという問題が残されている．

% \section{本研究の立場}
% \label{sec:our_position}

% 潜在コンテキストによる暗黙的適応は，
% 未知要因を広く包含できる可能性がある一方で，
% 推定された表現の解釈や
% 失敗要因の特定が困難になりやすい．
% 本研究は，実運用における
% 計画の信頼性と診断可能性を重視し，
% 環境変動を
% \emph{明示的に同定すべき対象}
% として扱う立場を取る．

% 具体的には，環境ダイナミクスの変化を
% 潜在表現に吸収するのではなく，
% 物理パラメータとして切り出して推定し，
% その推定結果を世界モデルへ直接反映する枠組みを採用する．
% この設計により，
% 計画精度の向上だけでなく，
% どの物理的要因が制御性能に影響を与えているのかを
% 定量的に分析することが可能となる．

\section{本研究の立場}
\label{sec:our_position}

本研究では，環境変動に対する適応を，
単に性能を維持するための経験的工夫としてではなく，
MBRLにおける計画性能劣化の原因を
構造的に切り分ける問題として捉える．
特に，物理パラメータの不一致が世界モデルの予測誤差を通じて
計画の破綻として顕在化する点に着目する．

この観点から，
環境ダイナミクスの変化を
潜在表現に吸収するのではなく，
\emph{明示的に同定すべき物理条件}として扱う立場を取る．
具体的には，真の物理パラメータが既知であるとする
Oracle条件下において，
計画性能が摂動に対して安定することをまず確認し，
計画が成立するための構造的条件を明らかにする．

一方で，実環境では真の物理パラメータに直接アクセスできないため，
その推定機構を導入し，
Oracle条件に近い世界モデルを構成することで
実用的な適応を実現する．
このように本研究は，
物理パラメータ推定そのものを目的とするのではなく，
計画が安定する条件を明示的に切り出し，
その条件を近似する枠組みとして推定を位置づける．


% 言いたいこととしては、ちゃんと推定できているからうまくいっている、
% 既存の研究では暗示的推定機構を加えることで、その潜在空間の分析などからこれを入れるとうまくいくだろう
% って言っているだけで、そこは定量評価を行う観点では限界が存在する。
% それに対して、oracleを導入していることで、明示的な物理条件の下では摂動に対して強いパフォーマンスが
% 得られる、そのうえでoracleにはアクセスができないから推定機構を加えている、という立場を強調したい。