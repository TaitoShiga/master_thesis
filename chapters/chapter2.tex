\chapter{研究背景}
\label{chap:background}
\section{強化学習の基礎理論}
\label{chap:rl}
強化学習(Reinforcement Learning:RL)は,エージェントが環境内での行動を通じてタスクを学習することを目的とした
一種の機械学習手法である\cite{sutton2018reinforcement}.このプロセスでは,エージェントは状態から行動を選択し,
各状態における状態遷移確率にしたがい次の状態へ遷移する.エージェントはそれぞれの遷移に対して報酬を受け取り,この報酬の最大化を目指す
最適化問題を解くことによって,どの行動系列が最も有益であるのかを学習する.
重要なのは,エージェントが特定の指令を受けることなく,試行錯誤を通じて環境と相互作用することによってのみ学習が進められることである.この過程
においてエージェントは即時に得られる報酬だけでなく,将来得られるであろう累積報酬予測を考慮して行動を選択しなければならない.強化学習
が扱う問題は多くの場合,マルコフ決定過程(Markov Decision Process: MDP)\cite{markov}としてモデル化された環境内で行動し,
エージェントはその環境からの十分な観測を通じて,タスクの遂行に適切な状態遷移を引き起こす行動をとる必要がある.
上で述べたMDPは,タプル$\mathcal{M}=(\mathcal{\mu_{0}}, \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$で表現される.
ここで,$\mathcal{\mu_{0}}$は初期状態空間を,$\mathcal{S}$は状態空間を,$\mathcal{A}$は行動空間を表す.$\mathcal{P}$は確率関数の集合で,
$\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$であり,$\mathcal{P}(s'|s,a)$は
状態$s$から行動$a$をとったときに状態$s'$へ遷移する確率を表す.また,$\mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$は報酬関数であり,
状態$s$から行動$a$をとったときに状態$s'$へ遷移するときに得られる報酬を表す.そして,$\gamma\in(0,1]$は割引率と呼ばれ,
将来得られる報酬の重要度を決定する.

エージェントは$\mathcal{M}$の中で,
状態$s$において行動$a$をとる確率が$\pi(a|s)$で与えられる確率的な方策$\pi:\mathcal{S}\rightarrow\mathcal{A}$に基づいて行動する.
あるMDP$\mathcal{M}$と方策$\pi$が与えられると状態価値関数$V_{\mathcal{M}}^{\pi}(s)$が以下の式(\ref{eq:V})で考えられる.
\begin{equation}
    V_{\mathcal{M}}^{\pi}(s) = \mathbb{E}\left[\sum_{t=0}^{T}\gamma^{t}r_{t}|\pi,s\right]
    \label{eq:V}
\end{equation}
ここで,$r_{t}$は時刻$t$における報酬,$T$はエージェントがタスクが完了するまで$\mathcal{M}$内で行動する回数,すなわちエピソードの長さを
表す.エピソードに終了条件を設定しないnon episodicなタスクの場合,$T=\infty$となる.
期待値$\mathbb{E}$は$s_0\sim\mu_0$,$a_i\sim\pi(\cdot|s_i)$,$s_{i+1}\sim\mathcal{T}(\cdot|s_i,a_i)$で取る.
状態価値関数は状態$s$からこの先方策$\pi$に基づいて行動した場合に得られるであろう割引累積報酬の期待値を表し,その状態にいる価値を推定する.
また,状態$s$から行動$a$を取ったときに期待される割引累積報酬を推定する行動価値関数$Q_{\mathcal{M}}^{\pi}(s,a)$が式(\ref{eq:Q})で導出できる.
\begin{equation}
    Q_{\mathcal{M}}^{\pi}(s,a) = \mathbb{E}_{s'\sim\mathcal{T}(\cdot|s,a)}[\mathcal{R}(s,a,s')+{\gamma}V_{\mathcal{M}}^{\pi}(s')]
    \label{eq:Q}
\end{equation}

以上からRLの基本となるゴールは,$Q_{\mathcal{M}}^{*}(s,a)=\sup\limits_{\pi}Q_{\mathcal{M}}^\pi(s,a)$
なる最適な状態および行動価値関数から最適な方策$\forall s \in \mathcal{S},\pi_{\mathcal{M}}^{*}(s)=\argmax\limits_{a \in \mathcal{A}}Q_{\mathcal{M}}^{*}(s,a)$
を得ることである.よって方策更新により最大化される目的関数$\mathcal{J}(\pi)$は
\begin{equation}
    \mathcal{J}(\pi) = \mathbb{E}_{(s,a)\sim\mu^{\pi}(s,a)}[\mathcal{R}(s,a,s')]
    \label{eq:pi}
\end{equation}
として表される.ここで$\mu^{\pi}(s,a)$は方策$\pi$に基づく定常状態行動確率分布であり,
この方策の下でエピソードにわたって行動を続けたときの,状態と行動のペアの長期的な分布を表す\cite{bellemare2017distributional}.
また,RLの問題は予測と方策改善の二つのフェーズに分けられる.予測の段階では現在の方策の質が評価され,方策改善フェーズでは
予測で評価された方策を改善するよう方策が調節される.RLアルゴリズムはこれらの二つのステップを反復して方策の最適化を行う.


% モデルベース強化学習の進展
% TODO: dreamerとかも引用して、tdmpc2に偏った議論を避けるようにする
\section{モデルベース強化学習の進展}
\label{sec:mbrl}

モデルベース強化学習（Model-Based Reinforcement Learning; MBRL）は，
環境の遷移ダイナミクス $p(s_{t+1} \mid s_t, a_t)$ を近似する学習モデル
$\hat{f}_{\theta}$ を構築し，これを利用して行動を決定する手法である\cite{kaelbling1996reinforcement}．
MBRLの最大の特徴は，学習済みモデル上で将来の状態遷移を予測し，
その予測に基づいた計画（Planning）が可能である点にある．
一般にMBRLは，経験を直接方策に反映させるモデルフリー手法と比較して
サンプル複雑性の面で優れており \cite{nagabandi2018neural,NIPS2014_c7c9344b,deisenroth2013gaussian}，
実機実験のコストや安全性が懸念されるロボティクス分野において
極めて重要な役割を果たす．

初期のMBRLにおいては，
ガウス過程（Gaussian Processes）\cite{deisenroth2011pilco} や
時間変化線形モデル \cite{levine2014learning}
といった比較的単純な関数近似器を用いたダイナミクス学習が主流であった．
特に PILCO \cite{deisenroth2011pilco} は，
確率的なダイナミクスモデルとモデルの不確実性を
長期的な計画に組み込むことで，
優れたサンプル効率を達成した．
しかし，これらの古典的な近似手法は，
高次元の状態空間や摩擦接触を含む複雑な非線形ダイナミクスを
正確に捉えることが困難であるという課題を有していた
\cite{nagabandi2018neural}．

これに対し，近年では深層ニューラルネットワーク
（Deep Neural Networks; DNN）を
高容量な関数近似器として導入することで，
高次元かつ複雑なダイナミクスを持つ
ロボット制御タスクへの適用が可能となっている．

DNNを用いたモデル学習においては，
高次元の観測値を直接予測する代わりに，
制御に必要な情報を凝縮した低次元の潜在空間
（Latent Space）上でダイナミクスを記述する手法が
主流となっている．
これにより，高次元観測に伴う計算コストの抑制と，
長期的なホライゾンにおける予測精度の維持を
両立している．

エージェントは各時刻 $t$ において，
学習されたモデルを用いて以下の最適化問題を解くことで
行動系列を決定する．
\begin{equation}
(a_t, \dots, a_{t+H-1})
=
\argmax_{a_t, \dots, a_{t+H-1}}
\sum_{t'=t}^{t+H-1}
\gamma^{t'-t}
r(s_{t'}, a_{t'})
\label{eq:mbrl_planning}
\end{equation}
ここで $H$ は予測ホライゾンを表す \cite{nagabandi2018neural}．

この枠組みの代表例である TD-MPC2 \cite{hansen2023td} は，
潜在空間上での状態遷移，報酬予測，
および価値関数の学習を統合し，
サンプリングベースの最適化手法を用いることで，
複雑な連続値制御タスクにおいて高い性能を示している．
TD-MPC2はマルチタスク学習においても優れた能力を発揮する一方で，
内部に保持する世界モデルは
学習時の環境パラメータに固執した
静的な表現になりやすく，
運用中に生じる物理的な摂動への適応には
課題を残している．

% モデルベース強化学習における環境の不確実性

\section{モデルベース強化学習における環境の不確実性}

MBRLは，\ref{sec:mbrl}で述べたように, 学習されたダイナミクスモデルに基づいて
将来の状態遷移を予測し，その予測結果を用いて計画を行う枠組みである．
このため，モデルの予測精度は制御性能に直接的かつ決定的な影響を及ぼす．
特に，複数ステップ先の状態遷移を考慮する計画型手法においては，
わずかな予測誤差であっても，その影響が時間とともに累積し，
行動選択の妥当性を大きく損なう可能性がある．

しかし，実環境への適用を考えた場合，
学習時に想定された環境と完全に同一の遷移ダイナミクスが
運用中も維持されるとは限らない．
ロボットの個体差，部品の摩耗や経年劣化，
あるいは接触条件や外乱といった環境側の要因により，
実際の遷移ダイナミクスは学習時とは異なるものとなり得る．

このようなダイナミクスのミスマッチが生じた場合，
学習済みモデルに基づく状態予測は徐々に現実から乖離していく．
とりわけ，モデル予測制御（Model Predictive Control; MPC）に代表される
計画型アプローチでは，
予測誤差が行動計画全体に波及し，
意図しない状態遷移やタスク失敗を引き起こす要因となる．
この問題は，接触を含む複雑なタスクや，
長い予測ホライゾンを必要とするタスクにおいて
より顕著に現れることが知られている\cite{nagabandi2018learning,lee2020context}．

近年の高性能なMBRL手法においても，
この不確実性の問題は本質的には未解決である．
多くの手法では，学習過程を通じて獲得された
単一の環境ダイナミクスを内部モデルとして保持し，
それを固定的に用いた計画が行われる．
その結果，運用中に生じる物理パラメータの変動や
環境条件の変化に対して，
モデルを動的に修正・適応する機構を十分に備えていない場合が多い．

このように，現在のモデルベース強化学習手法は，
環境が不変であるという暗黙の前提の下で設計されており，
パラメータが継続的に変動し得る実世界環境においては，
適応能力の不足が実用上の大きな課題となっている．

% 不確実性に対する既存の戦略

\section{不確実性に対する既存の戦略}
\label{sec:uncertainty}

前節で述べたように，モデルベース強化学習（MBRL）は，
学習されたダイナミクスモデルに基づいて計画を行うため，
環境ダイナミクスの変動に起因する予測誤差が
制御性能に直接的な影響を及ぼす．
このようなダイナミクスのミスマッチに対処するため，
これまでの研究では大きく分けて
\emph{頑健化（Robustness）}と
\emph{適応化（Adaptation）}
という二つの戦略が検討されてきた．

頑健化の代表的な手法として，
Domain Randomization（DR）が広く用いられている\cite{tobin2017domain}．
DR では，学習段階においてリンクの質量や摩擦係数などの
環境パラメータを一定の範囲内でランダムに変化させることで，
単一の方策が多様な環境条件下でも
平均的に良好な性能を発揮することを目指す．
このアプローチは実装が比較的容易であり，
Sim2Realの文脈においても一定の成功を収めてきた．

一方で，DR はあらゆる環境変動に対して
単一の方策で対応することを前提とするため，
特定の環境条件における最適性を犠牲にした
保守的な制御になりやすいという課題を有する．
また，想定された分布の外側にある変動や，
時間的に変化するパラメータに対しては，
十分な性能を維持できない場合も少なくない\cite{ding2021not}．

これに対し，実行時に環境の変化を検知し，
モデルや方策を動的に調整する
適応化のアプローチが提案されてきた．
この系譜には，過去の遷移履歴を用いて
モデルパラメータを更新するメタ学習的手法\cite{nagabandi2018learning}や，
リカレント構造の内部状態を更新することで
環境変化に追従する手法が含まれる\cite{duan2016rl}．

近年では，環境の特性を
「コンテキスト」と呼ばれる潜在変数として抽出し，
それを条件付け情報として
ダイナミクスモデルや方策に入力する
コンテキスト学習の枠組みが注目を集めている．
このような手法では，
未知の環境要因や複合的な変動を
潜在表現に吸収することで，
幅広い環境条件への適応が可能となる．
その結果，頑健化手法と比較して，
特定の環境に特化した鋭い制御を実現できる点が
大きな利点とされている．

しかし，多くの既存手法において，
抽出されるコンテキストは
物理的な意味を持たない潜在表現であり，
何が原因でダイナミクスが変化したのかを
明確に解釈することは困難である．
そのため，性能劣化が生じた際の
失敗要因の特定や，
実運用における信頼性評価・デバッグを
体系的に行うことが難しいという問題が残されている．

\section{本研究の立場}
\label{sec:our_position}

潜在コンテキストによる暗黙的適応は，
未知要因を広く包含できる可能性がある一方で，
推定された表現の解釈や
失敗要因の特定が困難になりやすい．
本研究は，実運用における
計画の信頼性と診断可能性を重視し，
環境変動を
\emph{明示的に同定すべき対象}
として扱う立場を取る．

具体的には，環境ダイナミクスの変化を
潜在表現に吸収するのではなく，
物理パラメータとして切り出して推定し，
その推定結果を世界モデルへ直接反映する枠組みを採用する．
この設計により，
計画精度の向上だけでなく，
どの物理的要因が制御性能に影響を与えているのかを
定量的に分析することが可能となる．
