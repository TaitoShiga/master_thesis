\chapter{研究背景}
\label{chap:background}

本章では，本研究の背景として，強化学習およびモデルベース強化学習（MBRL）の基本概念と，
実環境適用において顕在化する課題を整理する．
まず\ref{chap:rl}節で強化学習の基礎を概説する．
続いて\ref{sec:mbrl}節で，MBRLの枠組みと近年の進展を概観する．
その上で，\ref{sec:uncertainty_mbrl}節では，学習モデルに基づく計画という性質が，
環境変動下でどのような不確実性として現れるかを整理する．
さらに\ref{sec:existing_strategies}節で，不確実性に対する既存の戦略を概観し，
最後に\ref{sec:our_position}節で本研究の立場を明確にする．

\section{強化学習}
\label{chap:rl}

\subsection{基礎理論}
\label{chap:fund_rl}
強化学習(Reinforcement Learning:RL)は,エージェントが環境内での行動を通じてタスクを学習することを目的とした
一種の機械学習手法である\cite{sutton2018reinforcement}.このプロセスでは,エージェントは状態から行動を選択し,
各状態における状態遷移確率にしたがい次の状態へ遷移する.エージェントはそれぞれの遷移に対して報酬を受け取り,この報酬の最大化を目指す
最適化問題を解くことによって,どの行動系列が最も有益であるのかを学習する.
重要なのは,エージェントが特定の指令を受けることなく,試行錯誤を通じて環境と相互作用することによってのみ学習が進められることである.この過程
においてエージェントは即時に得られる報酬だけでなく,将来得られるであろう累積報酬予測を考慮して行動を選択しなければならない.強化学習
が扱う問題は多くの場合,マルコフ決定過程(Markov Decision Process: MDP)\cite{markov}としてモデル化された環境内で行動し,
エージェントはその環境からの十分な観測を通じて,タスクの遂行に適切な状態遷移を引き起こす行動をとる必要がある.
上で述べたMDPは,タプル$\mathcal{M}=(\mathcal{\mu_{0}}, \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$で表現される.
ここで,$\mathcal{\mu_{0}}$は初期状態空間を,$\mathcal{S}$は状態空間を,$\mathcal{A}$は行動空間を表す.$\mathcal{P}$は確率関数の集合で,
$\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$であり,$\mathcal{P}(s'|s,a)$は
状態$s$から行動$a$をとったときに状態$s'$へ遷移する確率を表す.また,$\mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$は報酬関数であり,
状態$s$から行動$a$をとったときに状態$s'$へ遷移するときに得られる報酬を表す.そして,$\gamma\in(0,1]$は割引率と呼ばれ,
将来得られる報酬の重要度を決定する.

エージェントは$\mathcal{M}$の中で,
状態$s$において行動$a$をとる確率が$\pi(a|s)$で与えられる確率的な方策$\pi:\mathcal{S}\rightarrow\mathcal{A}$に基づいて行動する.
あるMDP$\mathcal{M}$と方策$\pi$が与えられると状態価値関数$V_{\mathcal{M}}^{\pi}(s)$が以下の式(\ref{eq:V})で考えられる.
\begin{equation}
    V_{\mathcal{M}}^{\pi}(s) = \mathbb{E}\left[\sum_{t=0}^{T}\gamma^{t}r_{t}|\pi,s\right]
    \label{eq:V}
\end{equation}
ここで,$r_{t}$は時刻$t$における報酬,$T$はエージェントがタスクが完了するまで$\mathcal{M}$内で行動する回数,すなわちエピソードの長さを
表す.エピソードに終了条件を設定しないnon episodicなタスクの場合,$T=\infty$となる.
期待値$\mathbb{E}$は$s_0\sim\mu_0$,$a_i\sim\pi(\cdot|s_i)$,$s_{i+1}\sim\mathcal{T}(\cdot|s_i,a_i)$で取る.
状態価値関数は状態$s$からこの先方策$\pi$に基づいて行動した場合に得られるであろう割引累積報酬の期待値を表し,その状態にいる価値を推定する.
また,状態$s$から行動$a$を取ったときに期待される割引累積報酬を推定する行動価値関数$Q_{\mathcal{M}}^{\pi}(s,a)$が式(\ref{eq:Q})で導出できる.
\begin{equation}
    Q_{\mathcal{M}}^{\pi}(s,a) = \mathbb{E}_{s'\sim\mathcal{T}(\cdot|s,a)}[\mathcal{R}(s,a,s')+{\gamma}V_{\mathcal{M}}^{\pi}(s')]
    \label{eq:Q}
\end{equation}

以上からRLの基本となるゴールは,$Q_{\mathcal{M}}^{*}(s,a)=\sup\limits_{\pi}Q_{\mathcal{M}}^\pi(s,a)$
なる最適な状態および行動価値関数から最適な方策$\forall s \in \mathcal{S},\pi_{\mathcal{M}}^{*}(s)=\argmax\limits_{a \in \mathcal{A}}Q_{\mathcal{M}}^{*}(s,a)$
を得ることである.よって方策更新により最大化される目的関数$\mathcal{J}(\pi)$は
\begin{equation}
    \mathcal{J}(\pi) = \mathbb{E}_{(s,a)\sim\mu^{\pi}(s,a)}[\mathcal{R}(s,a,s')]
    \label{eq:pi}
\end{equation}
として表される.ここで$\mu^{\pi}(s,a)$は方策$\pi$に基づく定常状態行動確率分布であり,
この方策の下でエピソードにわたって行動を続けたときの,状態と行動のペアの長期的な分布を表す\cite{bellemare2017distributional}.
また,RLの問題は予測と方策改善の二つのフェーズに分けられる.予測の段階では現在の方策の質が評価され,方策改善フェーズでは
予測で評価された方策を改善するよう方策が調節される.RLアルゴリズムはこれらの二つのステップを反復して方策の最適化を行う.

\subsection{TD学習}
\label{sec:tderror}
Temporal Difference(TD)学習は,TD誤差と呼ばれる,1ステップ先の期待報酬の推定値と現在の期待報酬の推定値との差分
を用いて価値関数を更新する手法である\cite{sutton1988learning}.TD学習はモデルフリーな予測を行う
アルゴリズムであり,エージェントはMDPの情報が与えられない.よってエージェントはMDPのサンプルから
状態価値を推定する.状態価値関数の更新則は以下の式\ref{eq:td}の通りとなる.
\begin{equation}
    \label{eq:td}
    V_{\mathcal{M}}^{\pi}(s) \leftarrow V_{\mathcal{M}}^{\pi}(s) + \alpha(r_{t+1} + \gamma V_{\mathcal{M}}^{\pi}(s_{t+1}) - V_{\mathcal{M}}^{\pi}(s_{t}))
\end{equation}
ここで$\alpha$は学習率を表し,$r_{t+1}$は現在のステップ$t$で行動した結果,次のステップ$t+1$に遷移する際に得られた報酬である.
$r_{t+1}+\gamma V_{\mathcal{M}}^\pi$はTD目標と呼ばれ,今回のサンプリングで得られた値を,
$r_{t+1}+\gamma V_{\mathcal{M}}^\pi-V_{\mathcal{M}}^\pi$はTD誤差で,今回のサンプリングの値と前回までに得た値の差を表している.
TD学習の利点は,学習に状態価値関数の推定値を用いているため,エージェントがエピソードを完了していなくても
その状態から先の報酬を予測(ブートストラップ)し,学習を1ステップからでも行うことが可能である.

以上のように，価値推定と方策改善の枠組みは強化学習の基礎を成す．
しかし，実環境での試行錯誤には安全性やコストの制約が大きく，
限られたデータから効率的に学習する枠組みが必要となる．
この観点から，環境モデルを明示的に学習し計画に利用するモデルベース強化学習が検討されてきた．

% モデルベース強化学習の進展
% TODO: dreamerとかも引用して、tdmpc2に偏った議論を避けるようにする
\section{モデルベース強化学習の進展}
\label{sec:mbrl}

モデルベース強化学習（Model-Based Reinforcement Learning; MBRL）は，
環境の遷移ダイナミクス $p(s_{t+1} \mid s_t, a_t)$ を近似する学習モデル
$\hat{f}_{\theta}$ を構築し，これを利用して行動を決定する手法である\cite{kaelbling1996reinforcement}．
MBRLの最大の特徴は，学習済みモデル上で将来の状態遷移を予測し，
その予測に基づいた計画（Planning）が可能である点にある．
一般にMBRLは，経験を直接方策に反映させるモデルフリー手法と比較して
サンプル複雑性の面で優れており \cite{nagabandi2018neural,NIPS2014_c7c9344b,deisenroth2013gaussian}，
実機実験のコストや安全性が懸念されるロボティクス分野において
極めて重要な役割を果たす．

初期のMBRLにおいては，
ガウス過程（Gaussian Processes）\cite{deisenroth2011pilco} や
時間変化線形モデル \cite{levine2014learning}
といった比較的単純な関数近似器を用いたダイナミクス学習が主流であった．
特に PILCO \cite{deisenroth2011pilco} は，
確率的なダイナミクスモデルとモデルの不確実性を
長期的な計画に組み込むことで，
優れたサンプル効率を達成した．
しかし，これらの古典的な近似手法は，
高次元の状態空間や摩擦接触を含む複雑な非線形ダイナミクスを
正確に捉えることが困難であるという課題を有していた
\cite{nagabandi2018neural}．

これに対し，近年では深層ニューラルネットワーク
（Deep Neural Networks; DNN）を
高容量な関数近似器として導入することで，
高次元かつ複雑なダイナミクスを持つ
ロボット制御タスクへの適用が可能となっている．

DNNを用いたモデル学習においては，
高次元の観測値を直接予測する代わりに，
制御に必要な情報を凝縮した低次元の潜在空間
（Latent Space）上でダイナミクスを記述する手法が
主流となっている．
これにより，高次元観測に伴う計算コストの抑制と，
長期的なホライゾンにおける予測精度の維持を
両立している．

エージェントは各時刻 $t$ において，
学習されたモデルを用いて以下の最適化問題を解くことで
行動系列を決定する．
\begin{equation}
(a_t, \dots, a_{t+H-1})
=
\argmax_{a_t, \dots, a_{t+H-1}}
\sum_{t'=t}^{t+H-1}
\gamma^{t'-t}
r(s_{t'}, a_{t'})
\label{eq:mbrl_planning}
\end{equation}
ここで $H$ は予測ホライゾンを表す \cite{nagabandi2018neural}．

この枠組みの代表例である TD-MPC2 \cite{hansen2023td} は，
潜在空間上での状態遷移，報酬予測，
および価値関数の学習を統合し，
サンプリングベースの最適化手法を用いることで，
複雑な連続値制御タスクにおいて高い性能を示している．
TD-MPC2はマルチタスク学習においても優れた能力を発揮する一方で，
内部に保持する世界モデルは
学習時の環境パラメータに固執した
静的な表現になりやすく，
運用中に生じる物理的な摂動への適応には
課題を残している．

% モデルベース強化学習における環境の不確実性

\section{モデルベース強化学習における環境不確実性}

MBRLは，\ref{sec:mbrl}で述べたように, 学習されたダイナミクスモデルに基づいて
将来の状態遷移を予測し，その予測結果を用いて計画を行う枠組みである．
このため，モデルの予測精度は制御性能に直接的かつ決定的な影響を及ぼす．
特に，複数ステップ先の状態遷移を考慮する計画型手法においては，
わずかな予測誤差であっても，その影響が時間とともに累積し，
行動選択の妥当性を大きく損なう可能性がある．

しかし，実環境への適用を考えた場合，
学習時に想定された環境と完全に同一の遷移ダイナミクスが
運用中も維持されるとは限らない．
ロボットの個体差，部品の摩耗や経年劣化，
あるいは接触条件や外乱といった環境側の要因により，
実際の遷移ダイナミクスは学習時とは異なるものとなり得る．

このようなダイナミクスのミスマッチが生じた場合，
学習済みモデルに基づく状態予測は徐々に現実から乖離していく．
とりわけ，モデル予測制御（Model Predictive Control; MPC）に代表される
計画型アプローチでは，
予測誤差が行動計画全体に波及し，
意図しない状態遷移やタスク失敗を引き起こす要因となる．
この問題は，接触を含む複雑なタスクや，
長い予測ホライゾンを必要とするタスクにおいて
より顕著に現れることが知られている\cite{nagabandi2018learning,lee2020context}．

近年の高性能なMBRL手法においても，
この不確実性の問題は本質的には未解決である．
多くの手法では，学習過程を通じて獲得された
単一の環境ダイナミクスを内部モデルとして保持し，
それを固定的に用いた計画が行われる．
その結果，運用中に生じる物理パラメータの変動や
環境条件の変化に対して，
モデルを動的に修正・適応する機構を十分に備えていない場合が多い．

このように，現在のモデルベース強化学習手法は，
環境が不変であるという暗黙の前提の下で設計されており，
パラメータが継続的に変動し得る実世界環境においては，
適応能力の不足が実用上の大きな課題となっている．

% 不確実性に対する既存の戦略

\section{環境不確実性に対する既存の戦略}
\label{sec:existing_strategies}

前節で述べたように，モデルベース強化学習（MBRL）は，
学習されたダイナミクスモデルに基づいて計画を行うため，
環境ダイナミクスの変動に起因する予測誤差が
制御性能に直接的な影響を及ぼす．
このようなダイナミクスのミスマッチに対処するため，
これまでの研究では大きく分けて
\emph{頑健化（Robustness）}と
\emph{適応化（Adaptation）}
という二つの戦略が検討されてきた．

頑健化の代表的な手法として，
Domain Randomization（DR）が広く用いられている\cite{tobin2017domain}．
DR では，学習段階においてリンクの質量や摩擦係数などの
物理パラメータを一定の範囲内でランダムに変化させることで，
単一の方策が多様な環境条件下でも
平均的に良好な性能を発揮することを目指す．
このアプローチは実装が比較的容易であり，
Sim2Realの文脈においても一定の成功を収めてきた．

一方で，DR はあらゆる環境変動に対して
単一の方策で対応することを前提とするため，
特定の環境条件における最適性を犠牲にした
保守的な制御になりやすいという課題を有する．
また，想定された分布の外側にある変動や，
時間的に変化するパラメータに対しては，
十分な性能を維持できない場合も少なくない\cite{ding2021not}．

これに対し，実行時に環境の変化を検知し，
モデルや方策を動的に調整する
適応化のアプローチが提案されてきた．
この系譜には，過去の遷移履歴を用いて
モデルパラメータを更新するメタ学習的手法\cite{nagabandi2018learning}や，
リカレント構造の内部状態を更新することで
環境変化に追従する手法が含まれる\cite{duan2016rl}．

近年では，環境の特性を
「コンテキスト」と呼ばれる潜在変数として抽出し，
それを条件付け情報として
ダイナミクスモデルや方策に入力する
コンテキスト学習の枠組みが注目を集めている\cite{lee2020context}．
このような手法では，
未知の環境要因や複合的な変動を
潜在表現に吸収することで，
幅広い環境条件への適応が可能となる．
その結果，頑健化手法と比較して，
特定の環境に特化した鋭い制御を実現できる点が
大きな利点とされている．

しかし，多くの既存手法において，
抽出されるコンテキストは
物理的な意味を持たない潜在表現であり，
何が原因でダイナミクスが変化したのかを
明確に解釈することは困難である．
そのため，性能劣化が生じた際の
失敗要因の特定や，
実運用における信頼性評価・デバッグを
体系的に行うことが難しいという問題が残されている．

% \section{本研究の立場}
% \label{sec:our_position}

% 潜在コンテキストによる暗黙的適応は，
% 未知要因を広く包含できる可能性がある一方で，
% 推定された表現の解釈や
% 失敗要因の特定が困難になりやすい．
% 本研究は，実運用における
% 計画の信頼性と診断可能性を重視し，
% 環境変動を
% \emph{明示的に同定すべき対象}
% として扱う立場を取る．

% 具体的には，環境ダイナミクスの変化を
% 潜在表現に吸収するのではなく，
% 物理パラメータとして切り出して推定し，
% その推定結果を世界モデルへ直接反映する枠組みを採用する．
% この設計により，
% 計画精度の向上だけでなく，
% どの物理的要因が制御性能に影響を与えているのかを
% 定量的に分析することが可能となる．

\section{本研究の立場}
\label{sec:our_position}

本研究では，環境変動に対する適応を，
単に性能を維持するための経験的工夫としてではなく，
モデルベース強化学習における計画性能劣化の原因を
構造的に切り分ける問題として捉える．
特に，物理パラメータの不一致が世界モデルの予測誤差を通じて
計画の破綻として顕在化する点に着目する．

この観点から，
環境ダイナミクスの変化を
潜在表現に吸収するのではなく，
\emph{明示的に同定すべき物理条件}として扱う立場を取る．
具体的には，真の物理パラメータが既知であるとする
Oracle条件下において，
計画性能が摂動に対して安定することをまず確認し，
計画が成立するための構造的条件を明らかにする．

一方で，実環境では真の物理パラメータに直接アクセスできないため，
その近似として推定機構を導入し，
Oracle条件に近い世界モデルを構成することで
実用的な適応を実現する．
このように本研究は，
物理パラメータ推定そのものを目的とするのではなく，
計画が安定する条件を明示的に切り出し，
その条件を近似する枠組みとして推定を位置づける．


% 言いたいこととしては、ちゃんと推定できているからうまくいっている、
% 既存の研究では暗示的推定機構を加えることで、その潜在空間の分析などからこれを入れるとうまくいくだろう
% って言っているだけで、そこは定量評価を行う観点では限界が存在する。
% それに対して、oracleを導入していることで、明示的な物理条件の下では摂動に対して強いパフォーマンスが
% 得られる、そのうえでoracleにはアクセスができないから推定機構を加えている、という立場を強調したい。