\chapter{実験設定と評価方法}
\label{chap:experiment}

本章では，第\ref{chap:proposal}章で提案した明示的物理パラメータ推定に基づく適応的制御手法の有効性を検証するために用いた，実験環境，評価タスク，比較手法，および評価指標について述べる．本章では実験条件と評価手順のみを記述し，結果の解釈および考察は次章に譲る．


\section{実験目的と検証仮説}
\label{sec:exp_objective}

本研究の目的は，物理パラメータが未知な環境において，明示的な同定に基づく条件付き計画を導入することで，MBRLおよびMPC における計画の信頼性と制御性能が向上することを示すことである．この目的に基づき，以下の仮説を検証する．

\begin{itemize}
\item \textbf{H1}: 提案手法は，静的な未知環境において，Non-adaptive TD-MPC2 と比較して性能劣化を抑制できる．
\item \textbf{H2}: 提案手法は，真の物理パラメータを与えた Oracle TD-MPC2 に近い性能を示す．
\item \textbf{H3}: 提案手法は，Domain Randomization と比較して，評価時の固定パラメータ環境における汎化性能が高い，または同等の性能をより非保守的に達成する．
\item \textbf{H4}: 提案手法は，エピソード内で物理パラメータが変化する動的環境においても，変化に追従する挙動を示す．
\end{itemize}


\section{実装概要と再現性}
\label{sec:implementation_overview}

\subsection{シミュレーション基盤（DMControl）の概要}
\label{subsec:dmcontrol_overview}

本研究の実験は，MuJoCo 物理エンジンに基づく DeepMind Control Suite（DMControl）\cite{tassa2018deepmindcontrolsuite} を用いて行った．DMControl は，連続制御タスク群（例: Pendulum，Walker 等）を統一的な API で提供し，観測，行動，報酬の定義がタスクごとに明確に設計されている．本研究では，既存タスクをベースとして，物理パラメータの摂動を付与したタスクを実装し，未知環境への適応性能を評価した．

\subsection{再現性のための共通設定}
\label{subsec:reproducibility}

すべての手法は同一コードベース上に実装し，比較手法間で計算手順や実装上の差異が生じないようにした．各実験は複数の乱数シードにより独立に実行し，統計的に安定した評価を行った．学習率，バッチサイズ，MPPI のサンプル数等のハイパーパラメータは Appendix（表\ref{tab:hyperparameters}）にまとめる．

\subsection{タスクごとの学習ステップ数}
\label{subsec:task_steps}

タスクごとにダイナミクスの複雑性や探索難易度が異なるため，本研究ではタスクごとに学習ステップ数を設定した．ただし，同一タスク内では，提案手法および比較手法が同一の
を共有することで，公平な比較を行った．タスク別の学習ステップ数，シード数を表\ref{tab:task_protocol}に示す．

\begin{table}[t]
\centering
\caption{タスク別の学習および評価プロトコル}
\label{tab:task_protocol}
\begin{tabular}{lccc}
\toprule
\textbf{Task} & \textbf{Train steps} & \textbf{Seeds}\\
\midrule
Pendulum-Swingup & \texttt{(50000)} & \texttt{(5)} \\
Walker-Walk  & \texttt{(100000)} & \texttt{(5)}  \\
\bottomrule
\end{tabular}
\end{table}


\section{評価環境とタスク設計}
\label{sec:task_design}

\subsection{共通の MDP 設定}
\label{subsec:mdp}

各タスクは連続制御問題として定式化される．時刻 $t$ の観測 $\mathbf{o}_t$ と行動 $\mathbf{a}_t \in [-1,1]^{n_a}$ に基づき，環境からスカラー報酬 $r_t$ を得る．本研究では DMControl が提供する観測・行動・報酬の定義をそのまま用いた．


\subsection{評価タスク}
\label{subsec:evaluation_tasks}

本研究では，DMControl \cite{tassa2018deepmindcontrolsuite} が提供する2つの連続制御タスクを用いて評価を行う．各タスクの詳細仕様（観測空間・行動空間の定義，報酬関数の数式，物理パラメータのデフォルト値等）は Appendix \ref{sec:appendix_task_specs} に記載する．

\subsubsection{Pendulum-Swingup}
\label{subsubsec:pendulum}

Pendulum-Swingup は，倒立振子を下向き初期状態から振り上げ，上向き姿勢を維持する連続制御タスクである．観測は振子の鉛直方向からの角度（$\cos,\sin$ 表現）と角速度からなる3次元，行動は振子軸のトルクを表す1次元連続値である．報酬は上向き姿勢に近いほど高く設計されており，転倒による明示的な終了条件はない．


\subsubsection{Walker-Walk}
\label{subsubsec:walker_base}

Walker-Walk は，二次元平面上において二足歩行ロボットを前進歩行させる連続制御タスクである．ロボットは胴体（torso）および左右の股関節・膝関節・足首関節から構成され，動的バランスを維持しながら前進することが求められる．観測は姿勢・胴体高さ・速度からなる24次元，行動は6つの関節トルクを表す6次元連続値である．報酬は立位の維持と前進速度を同時に促すよう設計されている．転倒しても明示的な終了条件はなく，エピソードはタイムアウトまで継続する．


\section{物理パラメータ摂動と評価条件}
\label{sec:perturbation_and_conditions}

本研究では，学習時に用いる訓練環境（Randomized）と，汎化評価のための評価環境（Fixed / Dynamic）を実装し，未知環境への適応性能を検証する．

\subsection{Domain Randomization}
\label{subsec:dr_training}

Domain Randomization（DR）では，エピソードごとに物理パラメータをランダム化し，平均的に頑健な方策の学習を促す．本研究で用いたランダム化範囲を表\ref{tab:dr_ranges}に示す．いずれもエピソード内では固定とする．

\begin{table}[t]
\centering
\caption{DR におけるランダム化パラメータと範囲}
\label{tab:dr_ranges}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Parameter} & \textbf{Range (per episode)} \\
\midrule
Walker-Walk (Actuator) & actuator gear scale & $0.4$--$1.4 \times$ default \\
Pendulum-Swingup & pendulum mass & $0.5$--$2.5 \times$ default \\
\bottomrule
\end{tabular}
\end{table}

Walker-Walk の質量摂動は荷物運搬やペイロード変化を，actuator 摂動はモーター劣化やバッテリー消耗を模擬する．Pendulum-Swingup の質量摂動は慣性変化による振り上げダイナミクスの変動を表す．

\subsection{静的環境評価}
\label{subsec:fixed_eval}

学習済みモデルの汎化性能を評価するため，単一エピソード内で物理パラメータを固定した評価環境を複数用意する．DR 訓練範囲内（in-distribution）と範囲外（out-of-distribution）の両方で評価を行い，提案手法の汎化性能を検証する．

\subsection{動的環境評価（エピソード内変化）}
\label{subsec:dynamic_eval}

実環境では，バッテリー消耗や摩耗などにより，物理パラメータがエピソード内で時間変化する場合がある．そこで，Walker-Walk において actuator gear scale がエピソード内で線形に減衰する動的環境（初期値 $1.0$ から最終値 $0.4$ へ）を用意し，エピソード内適応性能を検証する．


\section{比較手法}
\label{sec:baselines}

提案手法の有効性を検証するため，以下の比較手法を用いる．

\begin{enumerate}
\item \textbf{Non-adaptive TD-MPC2}

推定器を用いず，標準の TD-MPC2 により制御を行う．未知環境における性能劣化の基準線として用いる．

\item \textbf{Domain Randomization}

DR 訓練環境で学習したモデルを，Fixed / Dynamic 環境で評価する．頑健化戦略との比較対象とする．

\item \textbf{Oracle TD-MPC2}

真の物理パラメータを制御器に与えた TD-MPC2 を用いる．達成可能な上限性能を示す参照点として用いる．
\end{enumerate}


\section{評価指標}
\label{sec:metrics}

制御性能の評価にはエピソード累積報酬を用いる．報告値として，外れ値に頑健な統計量である Interquartile Mean（IQM）を用いる\cite{agarwal2021deep}．必要に応じて，中央値および分位点も併記する（詳細は結果章で示す）．